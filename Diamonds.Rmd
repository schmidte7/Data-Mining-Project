---
title: "Diamonds"
author: "Francisco Arrieta, Emily Schmidt and Lucia Camenisch"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true             # creating a table of contents (toc)
    toc_float: 
      collapsed: false    # toc does not collapse and is shown as a sidebar (toc_float)
    number_sections: true # document sections are numbered
    theme: cosmo
---

```{=html}
<style>
body{
  color: #2F91AE;
  background-color: #F2F2F2;
}
pre{
  background-color: #96EAE3;
}
pre:not([class]){
  background-color: #15DDD8;
}
.toc-content{
  padding-left: 10px;
  padding-right: 10px;
}
.col-sm-8 {
  width: 75%;
}
code {
  color: #333333;
  background-color: #96EAE3;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.align = "center")
```

```{r Colors, include=FALSE}
#Aqua =         "#15DDD8"
#Dark Blue =    "#2F91AE"
#Yellow =       "#F9D53E"
#Light Gray =   "#F2F2F2"
#Light Aqua =   "#96EAE3"
```

```{r Libraries}
library(data.table)     #for reading data.tables
library(kableExtra)     #for more elaborate tables
library(ggplot2)        #for making graphs
library(GGally)         #for making graphs
library(dplyr)          #for data manipulation
library(tidyr)          #for changing the shape and hierarchy of a data set
library(DataExplorer)   #for graphing missing value percentages
library(car)            #for statistic functions
library(ellipse)        #for mapping correlation

#source("VIF.R")
```

# Data Exploration 

```{r Import Data}
diamonds <- fread("diamonds.csv", sep=",", header = T) # Load your data, diamonds.csv

diamonds$V1 <- NULL # Remove column 'V1' as it is similar to an ID variable - no additional meaning derived

# Rename columns for more precise names
colnames(diamonds)[5] <- "depth_ratio" # depth to depth_ratio
colnames(diamonds)[8] <- "length" # x to length
colnames(diamonds)[9] <- "width"  # y to width
colnames(diamonds)[10] <- "depth" # z to depth
```

## Dimension Summary 

```{r Data Exploration}
dim(diamonds) # Dimensions of data
summary(diamonds) # Produce result summaries of all variables
str(diamonds) # Type of variables

# Number of unique values in each variable
sapply(diamonds, function(x) length(unique(x)))
```

## Missing Values

```{r Missing Values}
# Missing values analysis
plot_missing(diamonds) # Plots the percentages of missing values

# pairs(diamonds[, c(1, 5:10)])
```


```{r Variables check}
# carat no problems
unique(diamonds$cut) # Review unique values for cut
diamonds$cut <- as.factor(diamonds$cut) # Factor the cut to five levels 
diamonds$cut <- ordered(diamonds$cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal")) # Ordered from worst to best

unique(diamonds$color) # Review unique values for color
diamonds$color <- as.factor(diamonds$color) # Factor the color to seven levels 
diamonds$color <- ordered(diamonds$color, levels = c("J", "I", "H", "G", "F", "E", "D")) # Ordered from worst to best

unique(diamonds$clarity) # Review unique values for clarity
diamonds$clarity <- as.factor(diamonds$clarity) # Factor the clarity to eight levels 
diamonds$clarity <- ordered(diamonds$clarity, levels = c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF")) # Ordered from worst to best

# table is ok

# price is ok

# Remove values of 0 for for dimensions which includes zeros in length and width
nrow(diamonds[depth %in% 0,]) # Remove 20 rows due to depth = 0.0
diamonds <- diamonds[depth > 0, ] # Include only values with depth greater than zero

# Create formula to check the absolute value of length to width, comparison 
diamonds[, subtraction := abs(length - width)]
nrow(diamonds[subtraction>10,]) # Remove 2 rows due their extreme subtraction value (~59 and ~26)
diamonds <- diamonds[subtraction <= 10, ] # Include only values with subtraction less than ten

diamonds[, depth_check := round(100*(2*depth)/((length + width)), 1)]
diamonds[, diff := abs(depth_check-depth_ratio)]
# treshold at 0.3? anastasia
nrow(diamonds[diff > 0.3,]) # we remove 268 rows
diamonds <- diamonds[diff <= 0.3,]
# hist(diamonds[diff >= 0.4 & diff < 1, diff], breaks = 50)

# Removed created columns needed to clean the data
diamonds[, subtraction := NULL]
diamonds[, depth_check := NULL]
diamonds[, diff := NULL]
# Total rows remove: 275 observations
```


```{r ordering cols}
# Reorder data table to group like variable types 
diamonds <- diamonds[, c(7, 2:4, 1, 8:10, 5:6)]
```

```{r,fig.width = 6, fig.height = 4, echo = FALSE, eval = FALSE}
#Used ggpairs to create a scatterplot matrix
ggpairs(diamonds[, c(1, 5:10)], title = "Scatterplot Matrix",
         proportions = "auto",
         columnLabels = c("Price", "Carat", "Length", "Width", "Depth","Depth Ratio","Table"),
         upper = list(continuous = wrap('cor',size = 3)),) + theme_light()
```

## Variable Visualisation

```{r Histograms}
diamonds %>% gather() %>% head() # Reshaping the data which means it collects a set of column names and places them into a single “key” column

histograms <- ggplot(gather(data = diamonds[, c(1, 5:10)]),aes(value)) +
  geom_histogram(aes(y=..density..),bins = 10, color = "white", fill = "blue") + # Creates bin sizing and sets the lines as white
  geom_density(alpha= .2, fill="#56B4E9") +
  facet_wrap(~key,scales = "free") + # Converting the graphs into panels
  ggtitle("Quantitative Variable Analysis") + # Title name
  ylab("Count") + xlab("Value") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines

histograms
```

```{r Correlation}
# Create heatmap to show variable correlation
cormat <- round(cor(diamonds[, c(1, 5:10)]),2) # Round the correlation coefficient to two decimal places

melted_cormat <- melt(cormat) # One way to reshape and elongate the data frame

# Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){ 
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }

# Rename the correlation coefficient value
upper_tri <- get_upper_tri(cormat)
upper_tri

# Use correlation between variables as distance
reorder_cormat <- function(cormat){ 
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}

# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)

# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)

# Create a ggheatmap with multiple characteristics 
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "purple", high = "blue", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  ggtitle("Correlation Heatmap") + # Title name
  theme_minimal() + # Minimal theme, keeps in the lines
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
  coord_fixed() +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2)

# Print the heat map
print(ggheatmap)
```

```{r Correlation Plot}
plotcorr(cor(diamonds[, -c(2:4)]))
```

```{r train-valid-test}
# set seed for reproducing the partition
set.seed(111)

# generating training set index
train.index <- sample(c(1:nrow(diamonds)), 0.5*nrow(diamonds))
# generating validation set index taken from the complementary of training set
valid.index <- sample(setdiff(c(1:nrow(diamonds)), train.index), 0.3*nrow(diamonds))
# defining test set index as complementary of (train.index + valid.index)
test.index <- as.numeric(setdiff(row.names(diamonds), union(train.index, valid.index)))
# creating data tables Train, Valid and Test using the indexes
Train <- diamonds[train.index, ]
Valid <- diamonds[valid.index, ]
Test <- diamonds[test.index, ]
```

# Dimension Reduction Analysis

```{r VIF}
#diamonds_lm <- lm(price ~ carat + length + width + depth + depth_ratio + table, data = diamonds)
#diamonds_lm2 <- lm(price ~ carat + depth_ratio + table, data = diamonds)
#diamonds_lm3 <- lm(price ~ carat + depth + depth_ratio + table, data = diamonds)

#diamonds_vif <- vif(diamonds_lm)
#VIF(diamonds[, c(5:8)])
#diamonds_vif2 <- vif(diamonds_lm2)

#diamonds_vif3 <- vif(diamonds_lm3)


#summary(diamonds_vif)
```


# Variable Prediction and Model Performance Evaluation

## Linear Regression

```{r}

```

## $k$-NN

```{r}
library(FNN)      #for finding k nearest neighbor
library(fastDummies)
library(caret)
library(forecast)
library(gridExtra)
```


```{r}
diamonds_dummies <- diamonds
diamonds_dummies <- dummy_cols(diamonds_dummies, 
                               select_columns = c("cut", "color", "clarity"), 
                               remove_selected_columns = TRUE)

#rename column to avoid issues with Neural net function
colnames(diamonds_dummies)[10] = "cut_Very_Good"

```


```{r}
#create data frames to normalize
knn_train <- diamonds_dummies[train.index,]
knn_valid <- diamonds_dummies[valid.index,]

knn_train_norm <- knn_train
knn_valid_norm <- knn_valid

# use preProcess() to normalize non-categorical variables
norm_values <- preProcess(knn_train[, c(1:7)], method=c("center", "scale"))

knn_train_norm[, c(1:7)] <- predict(norm_values, knn_train[, c(1:7)])
knn_valid_norm[,c(1:7)] <- predict(norm_values, knn_valid[, c(1:7)])
 

# computing kNN with knnreg from caret package
kNN = knnreg(x = knn_train_norm[, -c(1)] , y = knn_train_norm$price, k = 1)

# predicting prices of validation set on the validation data
knn_pred_y = predict(object = kNN, newdata = knn_valid_norm[, -c(1)])

# we have to scale back the price, to do so we fetch the mean and std value from norm.values
# dsplaying all means and stds
norm_values$mean
norm_values$std

# fetching for price
mean_price = norm_values$mean[1]
std_price = norm_values$std[1]

# scaling back (Y*mu + sigma)
knn_rescaled_prices = std_price * knn_pred_y + mean_price

# taking real prices of validation data from diamonds (which has not been touched -> original scale)
real_prices = diamonds[valid.index, price]

# computing accuracy measures
accuracy(object = knn_rescaled_prices, x = real_prices)
```

```{r Compute best K, }

#create a data frame to store accuracy results
accuracy_df <- data.frame(k = seq(1, 20, 1), accuracy = rep(0, 20))

# use validation data set to compute various values of Knn
for(i in 1:20) {
temp_knn_pred <- knnreg(x = knn_train_norm[, -c(1)] , y = knn_train_norm$price, k = i)

temp_knn_pred_y = predict(object = temp_knn_pred, newdata = knn_valid_norm[, -c(1)])
temp_knn_rescaled_prices = std_price * temp_knn_pred_y + mean_price

#update the accuracy table with results of confusion matrix accuracy from the loop
accuracy_df[i, 2] <- accuracy(object = temp_knn_rescaled_prices, x = real_prices)[2]
}

#display results
row_spec(kable_minimal(kbl(accuracy_df)), 4, bold = T, color = "white", background = "#78BE20") 

#plot results
ggplot (data = accuracy_df, aes(x = k, y = accuracy)) +
  geom_line (size = 1.2, color = "#0099F8") +
  geom_point(data= accuracy_df[4,], aes(x = k, y = accuracy), color = "#78BE20", size = 3) +
  labs(x = "Amount of Neighbors", y = "RMSE per Model", title = "Behavior of RMSE per amount of kNN")+
  theme_classic() 
```


```{r}

# computing kNN with knnreg from caret package
opt_kNN = knnreg(x = knn_train_norm[, -c(1)] , y = knn_train_norm$price, k = 4)

# predicting prices of validation set on the validation data
opt_knn_pred_y = predict(object = opt_kNN, newdata = knn_valid_norm[, -c(1)])

# scaling back (Y*mu + sigma)
opt_knn_rescaled_prices = std_price * opt_knn_pred_y + mean_price

# computing accuracy measures
opt_accuracy <- accuracy(object = opt_knn_rescaled_prices, x = real_prices)
opt_accuracy
```

```{r Plot KNN results, fig.height= 3, fig.width = 4}
plot1 <- data.frame("Real" = real_prices, 
                    "Predicted" = knn_rescaled_prices, 
                    "Color" = diamonds[valid.index, 3],
                    "Cut" = diamonds[valid.index, 2])
plot2 <- data.frame("Real" = real_prices, 
                    "Predicted" = opt_knn_rescaled_prices, 
                    "Color" = diamonds[valid.index, 3])


basic_knn_plot <- ggplot(data = plot1, 
                         aes(x= Real, y = Predicted, color = "Cut"), 
                         width = 5, height = 5)+
  geom_point()+
  labs(x= "Real Diamond Prices", y = 'Predicted Values')
basic_knn_plot

optimal_knn_plot <- ggplot(width = 5, height = 5)+
  geom_point(data = plot2, aes(x = Real, y = Predicted), fill = plot2$Color)+
  labs(x= "Real Diamond Prices", y = 'Predicted Values')
optimal_knn_plot

grid.arrange(basic_knn_plot, optimal_knn_plot, ncol = 2)

```




## Regression Tree

```{r}

```


## NeuralNetworks

### Loading Libraries

```{r Set Libraries}
library(forecast)        #for calculating RMSE
library(caret)          #for dummy variables
library(fastDummies)
library(keras)          #front-end library for neural networks
library(tensorflow)     #backend python library for neural network
library(magrittr)
tf$constant("Hello Tensorflow!")
```

### Data preprocessing

```{r Dummy ariables}
#create dummy columns 
diamonds_dummies <- dummy_cols(diamonds, select_columns = c("cut", "color", "clarity"))
diamonds_dummies <- diamonds_dummies[,-c(2:4)]

#rename column to avoid issues with Neural net function
colnames(diamonds_dummies)[10] = "cut_Very_Good"
```

### Partitioning

```{r Partitioning}
train_diamonds <- diamonds_dummies[train.index,]
valid_diamonds <- diamonds_dummies[valid.index,]
```

### Normalizing Data

```{r Normalize}
# use preProcess() to normalize non-categorical variables
norm_values <- preProcess(train_diamonds[1:7], method= c("range"))

train_diamonds_norm <- predict(norm_values, train_diamonds)
valid_diamonds_norm <- predict(norm_values, valid_diamonds)
```

### Model 1 (Layers = 1, Nodes = 1)

* Creating Model 1

```{r NN L1 N1 Model}
#create data sets to train and validate the model
#train
x_train <- c(t(train_diamonds_norm[, -c(1)]))
x_train <- as.array(x_train,
                    dim(t(train_diamonds_norm[, -c(1)])),
                    dimnames = list(rownames(x_train), colnames(x_train)))
x_train <- as_tensor(x_train, shape = dim(train_diamonds_norm[, -c(1)]))
y_train <- as_tensor(train_diamonds_norm$price)

#validation
x_valid <- c(t(valid_diamonds_norm[, -c(1)]))
x_valid <- as.array(x_valid,
                    dim(t(valid_diamonds_norm[, -c(1)])),
                    dimnames = list(rownames(x_valid), colnames(x_valid)))
x_valid <- as_tensor(x_valid, shape = dim(valid_diamonds_norm[, -c(1)]))
y_valid <- as_tensor(valid_diamonds_norm$price)

#create model
rm(model_1_1) #to prevent retraining of existing model
tf$random$set_seed(111)
model_1_1 <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(1, activation = "sigmoid", name = "HiddenLayer") %>%  # 1st hidden layer (26 nodes)
  layer_dense(1, activation = "sigmoid", name = "outputLayer")     # output layer (1 node)
  

#compile the model to be trained            
model_1_1 %>% compile(
  optimizer = optimizer_adam(),
  loss = loss_mean_squared_error(),
  metric = metric_root_mean_squared_error()
  )

#train the model and find the lowest rmse
history <- model_1_1 %>% fit(
    x = x_train,
    y = y_train,
    validation_data = c(x_valid, y_valid),
    verbose = FALSE
    )
```

* Measuring Error Model 1

```{r L1 N1 Pred}
# prediction values of validation set
y_valid_pred <- predict(model_1_1, x_valid)

# fetching a and b from standardization for scaling back price
a <- norm_values$ranges[1,1]
b <- norm_values$ranges[2,1]

valRescale <- function (x) { 
  value <- (x * (b-a) + a)
  return(value)
}

# scaling back price predictions
y_valid_pred <- valRescale(y_valid_pred)

# we have to change the class of y_valid_pred
class(y_valid_pred)
y_valid_pred <- as.numeric(y_valid_pred)
class(y_valid_pred)

# taking real values of validation set from original data (which wasn't standardized!)
y_valid_real <- diamonds[valid.index, price]

# copmuting accuracy measures of validation set
acc_1 <- accuracy(object = y_valid_pred, x = y_valid_real)
acc_1

```

### Model 2 (Layers = 1, Nodes = 26)

* Creating Model 2

```{r L1 N26 Model}
#create model
rm(model_1_26) #to prevent retraining of existing model
tf$random$set_seed(111)
model_1_26 <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(26, activation = "sigmoid", name = "HiddenLayer") %>%  # 1st hidden layer (26 nodes)
  layer_dense(1, activation = "sigmoid", name = "outputLayer")     # output layer (1 node)
  

#compile the model to be trained            
model_1_26 %>% compile(
  optimizer = optimizer_adam(),
  loss = loss_mean_squared_error(),
  metric = metric_root_mean_squared_error()
  )

#train the model and find the lowest rmse
history <- model_1_26 %>% fit(
    x = x_train,
    y = y_train,
    validation_data = c(x_valid, y_valid),
    verbose = FALSE
    )
```

* Measuring Error Model 2

```{r L1 N26 Pred}
# prediction values of validation set
y_valid_pred_1_26 <- predict(model_1_26, x_valid)

# scaling back price predictions
y_valid_pred_1_26 <- valRescale(y_valid_pred_1_26)

# we have to change the class of y_valid_pred
y_valid_pred_1_26 <- as.numeric(y_valid_pred_1_26)

# copmuting accuracy measures of validation set
acc_2 <- accuracy(object = y_valid_pred_1_26, x = y_valid_real)
acc_2
```

### Model 3 (Layers = 1, Nodes = 13)

* Creating Model 3

```{r L1 N26 Model}
#create model
rm(model_1_13) #to prevent retraining of existing model
tf$random$set_seed(111)
model_1_13 <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(13, activation = "sigmoid", name = "HiddenLayer") %>%  # 1st hidden layer (26 nodes)
  layer_dense(1, activation = "sigmoid", name = "outputLayer")     # output layer (1 node)
  

#compile the model to be trained            
model_1_13 %>% compile(
  optimizer = optimizer_adam(),
  loss = loss_mean_squared_error(),
  metric = metric_root_mean_squared_error()
  )

#train the model and find the lowest rmse
history <- model_1_13 %>% fit(
    x = x_train,
    y = y_train,
    validation_data = c(x_valid, y_valid),
    verbose = FALSE
    )
```

* Measuring Error Model 3

```{r L1 N26 Pred}
# prediction values of validation set
y_valid_pred_1_13 <- predict(model_1_13, x_valid)

# scaling back price predictions
y_valid_pred_1_13 <- valRescale(y_valid_pred_1_13)

# we have to change the class of y_valid_pred
y_valid_pred_1_13 <- as.numeric(y_valid_pred_1_13)

# copmuting accuracy measures of validation set
acc_3 <- accuracy(object = y_valid_pred_1_13, x = y_valid_real)
acc_3
```

### Model 4 (Layers = 2, Nodes = 26)

* Creating Model 4

```{r L2 N26 Model}
#create model
rm(model_2_26) #to prevent retraining of existing model
tf$random$set_seed(111)
model_2_26 <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(26, activation = "sigmoid", name = "HiddenLayer") %>%  # 1st hidden layer (26 nodes)
  layer_dense(26, activation = "sigmoid", name = "HiddenLayer2") %>%  # 1st hidden layer (26 nodes)
  layer_dense(1, activation = "sigmoid", name = "outputLayer")     # output layer (1 node)
  

#compile the model to be trained            
model_2_26 %>% compile(
  optimizer = optimizer_adam(),
  loss = loss_mean_squared_error(),
  metric = metric_root_mean_squared_error()
  )

#train the model and find the lowest rmse
history <- model_2_26 %>% fit(
    x = x_train,
    y = y_train,
    validation_data = c(x_valid, y_valid),
    verbose = FALSE
    )
```

* Measuring Error Model 4

```{r L2 N26 Pred}
# prediction values of validation set
y_valid_pred_2_26 <- predict(model_2_26, x_valid)

# scaling back price predictions
y_valid_pred_2_26 <- valRescale(y_valid_pred_2_26)

# we have to change the class of y_valid_pred
y_valid_pred_2_26 <- as.numeric(y_valid_pred_2_26)

# copmuting accuracy measures of validation set
acc_4 <- accuracy(object = y_valid_pred_2_26, x = y_valid_real)
acc_4
```


### Model 5 with Initializer (Layers = 2, Nodes = 26, GlorotNormal)

* Creating Model 5

```{r L2 N26 Glot Model}
#create model
rm(model_2_26G) #to prevent retraining of existing model
tf$random$set_seed(111)
model_2_26G <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(26, activation = "sigmoid", name = "HiddenLayer", kernel_initializer = "GlorotNormal") %>%  # 1st hidden layer (26 nodes)
  layer_dense(26, activation = "sigmoid", name = "HiddenLayer2", kernel_initializer = "GlorotNormal") %>%  # 1st hidden layer (26 nodes)
  layer_dense(1, activation = "sigmoid", name = "outputLayer")     # output layer (1 node)
  

#compile the model to be trained            
model_2_26G %>% compile(
  optimizer = optimizer_adam(),
  loss = loss_mean_squared_error(),
  metric = metric_root_mean_squared_error()
  )

#train the model and find the lowest rmse
history <- model_2_26G %>% fit(
    x = x_train,
    y = y_train,
    validation_data = c(x_valid, y_valid),
    verbose = FALSE
    )
```

* Measuring Error Model 5

```{r L2 N26 Glot Pred}
# prediction values of validation set
y_valid_pred_2_26G <- predict(model_2_26G, x_valid)

# scaling back price predictions
y_valid_pred_2_26G <- valRescale(y_valid_pred_2_26G)

# we have to change the class of y_valid_pred
y_valid_pred_2_26G <- as.numeric(y_valid_pred_2_26G)

# copmuting accuracy measures of validation set
acc_5 <- accuracy(object = y_valid_pred_2_26G, x = y_valid_real)
acc_5
```


### Model 6 with Initializer and Learning Rate (Layers = 2, Nodes = 26, GlorotNormal, LR = 0.005)

* Creating model 6

```{r L2 N26 Glot LR Model}
#create model
rm(model_2_26GLR) #to prevent retraining of existing model
tf$random$set_seed(111)
model_2_26GLR <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(26, activation = "sigmoid", name = "HiddenLayer", kernel_initializer = "GlorotNormal") %>%  # 1st hidden layer (26 nodes)
  layer_dense(26, activation = "sigmoid", name = "HiddenLayer2", kernel_initializer = "GlorotNormal") %>%  # 1st hidden layer (26 nodes)
  layer_dense(1, activation = "sigmoid", name = "outputLayer")     # output layer (1 node)
  

#compile the model to be trained            
model_2_26GLR %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.009),
  loss = loss_mean_squared_error(),
  metric = metric_root_mean_squared_error()
  )

#train the model and find the lowest rmse
history <- model_2_26GLR %>% fit(
    x = x_train,
    y = y_train,
    validation_data = c(x_valid, y_valid),
    verbose = FALSE
    )
```

* Measuring Error Model 6
 
```{r L2 N26 Glot LR Pred}
# prediction values of validation set
y_valid_pred_2_26GLR <- predict(model_2_26GLR, x_valid)

# scaling back price predictions
y_valid_pred_2_26GLR <- valRescale(y_valid_pred_2_26GLR)

# we have to change the class of y_valid_pred
y_valid_pred_2_26GLR <- as.numeric(y_valid_pred_2_26GLR)

# copmuting accuracy measures of validation set
acc_6 <- accuracy(object = y_valid_pred_2_26GLR, x = y_valid_real)
acc_6
```

### Create Neural Net Summary Table

```{r}
NN_res <- data.frame("Model" = c("L1 N1",
                                 "L1 N26",
                                 "L1 N13",
                                 "L2 N26",
                                 "L2 N26 G",
                                 "L2 N26 G LR"),
                     "Me" = "",
                     "RMSE" = "",
                     "MAE"= "",
                     "MPE" = "",
                     "MAPE" = "")

rownames(NN_res) <- NN_res$Model
NN_res$Model = NULL

str(round(get(paste("acc_", "1", sep ="")),2))

for (i in 1:6) {
  NN_res[i,]= round(get(paste("acc_", i, sep ="")),2)
}

kable_styling(kable(NN_res, full_width = FALSE))

RMSEplotdata <- t(NN_res$RMSE)
colnames(RMSEplotdata) <- t(rownames(NN_res))
barplot(RMSEplotdata, col = "yellow")

```



## Ensembles

```{r}

```

# Model Performance Summary

# Conclusions

