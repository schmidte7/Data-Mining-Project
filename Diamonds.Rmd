---
title: "Diamonds"
author: "Francisco Arrieta, Emily Schmidt and Lucia Camenisch"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true             # creating a table of contents (toc)
    toc_float: 
      collapsed: false    # toc does not collapse and is shown as a sidebar (toc_float)
    number_sections: true # document sections are numbered
    theme: cosmo
---

```{=html}
<style>
body{
  color: #2F91AE;
  background-color: #F2F2F2;
}
pre{
  background-color: #96EAE3;
}
pre:not([class]){
  background-color: #15DDD8;
}
.toc-content{
  padding-left: 10px;
  padding-right: 10px;
}
.col-sm-8 {
  width: 75%;
}
code {
  color: #333333;
  background-color: #96EAE3;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.align = "center")
```

```{r Colors, include=FALSE}
#Aqua =         "#15DDD8"
#Dark Blue =    "#2F91AE"
#Yellow =       "#F9D53E"
#Light Gray =   "#F2F2F2"
#Light Aqua =   "#96EAE3"
```

```{r Libraries}
library(data.table)     #for reading data.tables
library(kableExtra)     #for more elaborate tables
library(ggplot2)        #for making graphs
library(GGally)         #for making graphs
library(dplyr)          #for data manipulation
library(tidyr)          #for changing the shape and hierarchy of a data set
library(DataExplorer)   #for graphing missing value percentages
library(car)            #for statistic functions
library(ellipse)        #for mapping correlation

#source("VIF.R")
```

# Data Exploration 

```{r Import Data}
diamonds <- fread("diamonds.csv", sep=",", header = T) # Load your data, diamonds.csv

diamonds$V1 <- NULL # Remove column 'V1' as it is similar to an ID variable - no additional meaning derived

# Rename columns for more precise names
colnames(diamonds)[5] <- "depth_ratio" # depth to depth_ratio
colnames(diamonds)[8] <- "length" # x to length
colnames(diamonds)[9] <- "width"  # y to width
colnames(diamonds)[10] <- "depth" # z to depth
```

## Dimension Summary 

```{r Data Exploration}
dim(diamonds) # Dimensions of data
summary(diamonds) # Produce result summaries of all variables
str(diamonds) # Type of variables

# Number of unique values in each variable
sapply(diamonds, function(x) length(unique(x)))
```

## Missing Values

```{r Missing Values}
# Missing values analysis
plot_missing(diamonds) # Plots the percentages of missing values

# pairs(diamonds[, c(1, 5:10)])
```


```{r Variables check}
# carat no problems
unique(diamonds$cut) # Review unique values for cut
diamonds$cut <- as.factor(diamonds$cut) # Factor the cut to five levels 
diamonds$cut <- ordered(diamonds$cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal")) # Ordered from worst to best

unique(diamonds$color) # Review unique values for color
diamonds$color <- as.factor(diamonds$color) # Factor the color to seven levels 
diamonds$color <- ordered(diamonds$color, levels = c("J", "I", "H", "G", "F", "E", "D")) # Ordered from worst to best

unique(diamonds$clarity) # Review unique values for clarity
diamonds$clarity <- as.factor(diamonds$clarity) # Factor the clarity to eight levels 
diamonds$clarity <- ordered(diamonds$clarity, levels = c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF")) # Ordered from worst to best

# table is ok

# price is ok

# Remove values of 0 for for dimensions which includes zeros in length and width
nrow(diamonds[depth %in% 0,]) # Remove 20 rows due to depth = 0.0
diamonds <- diamonds[depth > 0, ] # Include only values with depth greater than zero

# Create formula to check the absolute value of length to width, comparison 
diamonds[, subtraction := abs(length - width)]
nrow(diamonds[subtraction>10,]) # Remove 2 rows due their extreme subtraction value (~59 and ~26)
diamonds <- diamonds[subtraction <= 10, ] # Include only values with subtraction less than ten

diamonds[, depth_check := round(100*(2*depth)/((length + width)), 1)]
diamonds[, diff := abs(depth_check-depth_ratio)]
# treshold at 0.3? anastasia
nrow(diamonds[diff > 0.3,]) # we remove 268 rows
diamonds <- diamonds[diff <= 0.3,]
# hist(diamonds[diff >= 0.4 & diff < 1, diff], breaks = 50)

# Removed created columns needed to clean the data
diamonds[, subtraction := NULL]
diamonds[, depth_check := NULL]
diamonds[, diff := NULL]
# Total rows remove: 275 observations
```


```{r ordering cols}
# Reorder data table to group like variable types 
diamonds <- diamonds[, c(7, 2:4, 1, 8:10, 5:6)]
```

```{r,fig.width = 6, fig.height = 4, echo = FALSE, eval = FALSE}
#Used ggpairs to create a scatterplot matrix
ggpairs(diamonds[, c(1, 5:10)], title = "Scatterplot Matrix",
         proportions = "auto",
         columnLabels = c("Price", "Carat", "Length", "Width", "Depth","Depth Ratio","Table"),
         upper = list(continuous = wrap('cor',size = 3)),) + theme_light()
```

## Variable Visualisation

```{r Histograms}
diamonds %>% gather() %>% head() # Reshaping the data which means it collects a set of column names and places them into a single “key” column

histograms <- ggplot(gather(data = diamonds[, c(1, 5:10)]),aes(value)) +
  geom_histogram(aes(y=..density..),bins = 10, color = "white", fill = "blue") + # Creates bin sizing and sets the lines as white
  geom_density(alpha= .2, fill="#56B4E9") +
  facet_wrap(~key,scales = "free") + # Converting the graphs into panels
  ggtitle("Quantitative Variable Analysis") + # Title name
  ylab("Count") + xlab("Value") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines

histograms
```

```{r Correlation}
# Create heatmap to show variable correlation
cormat <- round(cor(diamonds[, c(1, 5:10)]),2) # Round the correlation coefficient to two decimal places

melted_cormat <- melt(cormat) # One way to reshape and elongate the data frame

# Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){ 
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }

# Rename the correlation coefficient value
upper_tri <- get_upper_tri(cormat)
upper_tri

# Use correlation between variables as distance
reorder_cormat <- function(cormat){ 
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}

# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)

# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)

# Create a ggheatmap with multiple characteristics 
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "purple", high = "blue", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  ggtitle("Correlation Heatmap") + # Title name
  theme_minimal() + # Minimal theme, keeps in the lines
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
  coord_fixed() +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2)

# Print the heat map
print(ggheatmap)
```

```{r Correlation Plot}
plotcorr(cor(diamonds[, -c(2:4)]))
```

```{r train-valid-test}
# set seed for reproducing the partition
set.seed(111)

# generating training set index
train.index <- sample(c(1:nrow(diamonds)), 0.5*nrow(diamonds))
# generating validation set index taken from the complementary of training set
valid.index <- sample(setdiff(c(1:nrow(diamonds)), train.index), 0.3*nrow(diamonds))
# defining test set index as complementary of (train.index + valid.index)
test.index <- as.numeric(setdiff(row.names(diamonds), union(train.index, valid.index)))
# creating data tables Train, Valid and Test using the indexes
Train <- diamonds[train.index, ]
Valid <- diamonds[valid.index, ]
Test <- diamonds[test.index, ]
```

# Dimension Reduction Analysis

```{r VIF}
#diamonds_lm <- lm(price ~ carat + length + width + depth + depth_ratio + table, data = diamonds)
#diamonds_lm2 <- lm(price ~ carat + depth_ratio + table, data = diamonds)
#diamonds_lm3 <- lm(price ~ carat + depth + depth_ratio + table, data = diamonds)

#diamonds_vif <- vif(diamonds_lm)
#VIF(diamonds[, c(5:8)])
#diamonds_vif2 <- vif(diamonds_lm2)

#diamonds_vif3 <- vif(diamonds_lm3)


#summary(diamonds_vif)
```


# Variable Prediction and Model Performance Evaluation

## Linear Regression

```{r}

```

## $k$-NN

```{r}
library(FNN)      #for finding k nearest neighbor

```


```{r}
diamonds_dummies <- diamonds
diamonds_dummies <- dummy_cols(diamonds_dummies, select_columns = c("cut", "color", "clarity"))
diamonds_dummies <- diamonds_dummies[,-c(2:4)]

#rename column to avoid issues with Neural net function
colnames(diamonds_dummies)[10] = "cut_Very_Good"

```


```{r}
#create data frames to normalize
knn_train <- diamonds_dummies[train.index,]
knn_valid <- diamonds_dummies[valid.index,]

knn_train_norm <- knn_train
knn_valid_norm <- knn_valid

# use preProcess() to normalize non-categorical variables
norm_values <- preProcess(knn_train[, c(1:7)], method=c("center", "scale"))

knn_train_norm[, c(1:7)] <- predict(norm_values, knn_train[, c(1:7)])
knn_valid_norm[,c(1:7)] <- predict(norm_values, knn_valid[, c(1:7)])

# Compute k=1 nearest neighbor 
nn1 <- FNN::knn.reg(train = knn_train_norm[, -1], 
          test = knn_train_norm[, -1],
          y= knn_train_norm[, 1],
          k = 1)

length(knn_train_norm[, -1])
length(knn_train_norm[, 1])


nn1$
summary(nn1)

#display results
row.names(knn_train)[attr(nn1, "nn.index")]
nn1
```


## Regression Tree

```{r}

```


## NeuralNetworks

```{r Set Libraries}
library(forecast)        #for calculating RMSE
library(caret)          #for dummy variables
library(fastDummies)
library(keras)          #front-end library for neural networks
library(tensorflow)     #backend python library for neural network
library(magrittr)
```


```{r Dummy ariables}
#create dummy columns 
diamonds_dummies <- dummy_cols(diamonds, select_columns = c("cut", "color", "clarity"))
diamonds_dummies <- diamonds_dummies[,-c(2:4)]

#rename column to avoid issues with Neural net function
colnames(diamonds_dummies)[10] = "cut_Very_Good"
```

```{r Partitioning}
train_diamonds <- diamonds_dummies[train.index,]
valid_diamonds <- diamonds_dummies[valid.index,]
```


```{r Normalize}
# use preProcess() to normalize non-categorical variables
norm_values <- preProcess(train_diamonds[1:7], method= c("range"))

train_diamonds_norm <- predict(norm_values, train_diamonds)
valid_diamonds_norm <- predict(norm_values, valid_diamonds)
```


```{r Keras}
#create data sets to train and validate the model
#train
x_train <- c(t(train_diamonds_norm[, -c(1)]))
x_train <- as.array(x_train,
                    dim(t(train_diamonds_norm[, -c(1)])),
                    dimnames = list(rownames(x_train), colnames(x_train)))
x_train <- as_tensor(x_train, shape = dim(train_diamonds_norm[, -c(1)]))
y_train <- as_tensor(train_diamonds_norm$price)

#validation
x_valid <- c(t(valid_diamonds_norm[, -c(1)]))
x_valid <- as.array(x_valid,
                    dim(t(valid_diamonds_norm[, -c(1)])),
                    dimnames = list(rownames(x_valid), colnames(x_valid)))
x_valid <- as_tensor(x_valid, shape = dim(valid_diamonds_norm[, -c(1)]))
y_valid <- as_tensor(valid_diamonds_norm$price)

#create model
rm(model) #to prevent retraining of existing model
tf$random$set_seed(111)
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(26, activation = "sigmoid", name = "HiddenLayer") %>%  # 1st hidden layer (26 nodes)
  layer_dense(1, activation = "sigmoid", name = "outputLayer")       # output layer (1 node)

#compile the model to be trained            
model %>% compile(
  optimizer = optimizer_adam(),
  loss = loss_mean_squared_error(),
  metric = metric_root_mean_squared_error()
  )

#train the model and find the lowest rmse
history <- model %>% fit(
    x = x_train,
    y = y_train,
    validation_data = c(x_valid, y_valid)
    )
```


```{r}
predictions <- predict(model, train_diamonds)
predictions
```



```{r Parallel Computing}
detectCores()
getDoParWorkers()
cl <- makeCluster(4)
registerDoParallel(cl)
set.seed(1)

NNN <- cmpfun(neuralnet)
system.time(
#create neural network
diaNeuralNet <-neuralnet(price ~ .,
                          data = train_diamonds_norm,
                          linear.output = TRUE,
                          hidden = 1)
)

stopCluster(cl)
```


## Neural Network with Layers = 1 and Nodes = 2

```{r NN Layers 1 Nodes 2}
# create neural network plot
plot(diaNeuralNet, rep="best", show.weights = T, col.entry = "#00C1D5", col.hidden = "#387C2C", col.out = "#7F35B2", intercept = F)

# predict value

```


## Measuring errors (Layers = 1, Nodes = 2)

```{r RMSE Layers 1 Nodes 2}
#a function to convert normalized values to original values
valRescale <- function (x) { 
  value <- (x * (max(train_diamonds$price)- min(diamonds$price))) + min(diamonds$price) 
  return(value)
}

# predict values of Data using Neural net
dia_train_pred <- neuralnet::compute(diaNeuralNet, train_diamonds_norm)$net.result


# Calculate Root Mean Square Error (RMSE) using Metrics package
rs_actual_1 <- valRescale(train_diamonds_norm$price)
rs_pred_1 <- valRescale(dia_train_pred)

#calculate RMSE
RMSE_T_1 <- rmse(rs_actual_1, rs_pred_1)

#calculate for Validation data
dia_valid_pred <- neuralnet::compute(diaNeuralNet, valid_diamonds_norm)$net.result

rs_actual_V_1 <- valRescale(valid_diamonds_norm$price)
rs_pred_V_1 <- valRescale(dia_valid_pred)

RMSE_V_1 <- rmse(rs_actual_V_1, rs_pred_V_1)

#display Results
RMSE_T_1
RMSE_V_1
```


## Ensembles

```{r}

```

# Model Performance Summary

# Conclusions

