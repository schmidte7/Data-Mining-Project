---
title: "Diamonds"
author: "Francisco Arrieta, Emily Schmidt and Lucia Camenisch"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true             # creating a table of contents (toc)
    toc_float: 
      collapsed: false    # toc does not collapse and is shown as a sidebar (toc_float)
    number_sections: true # document sections are numbered
    theme: cosmo
---

```{=html}
<style>
body{
  color: #2F91AE;
  background-color: #F2F2F2;
}
pre{
  background-color: #96EAE3;
}
pre:not([class]){
  background-color: #15DDD8;
}
.toc-content{
  padding-left: 10px;
  padding-right: 10px;
}
.col-sm-8 {
  width: 75%;
}
code {
  color: #333333;
  background-color: #96EAE3;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.align = "center")
```

```{r Colors, include=FALSE}
#Aqua =         "#15DDD8"
#Dark Blue =    "#2F91AE"
#Yellow =       "#F9D53E"
#Light Gray =   "#F2F2F2"
#Light Aqua =   "#96EAE3"
```

```{r Libraries}
library(data.table)     #for reading data.tables
library(kableExtra)     #for more elaborate tables
library(ggplot2)        #for making graphs
library(GGally)         #for making graphs
library(dplyr)          #for data manipulation
library(tidyr)          #for changing the shape and hierarchy of a data set
library(DataExplorer)   #for graphing missing value percentages
library(car)            #for statistic functions
library(ellipse)        #for mapping correlation
library(naniar)         #for missing values 
library(RColorBrewer)   #for graph colors
library(rattle)         #Graphical Data Interface

library(rpart)          #for regression trees
library(rpart.plot)     #for plot trees
library(forecast)       #for accuracy
library(ggplot2)        #for visualizations
library(caret)          #for variable importance
library(randomForest)   #for randomForest
library(gbm)            #for boosting
library(ipred)          #for bagging

source("VIF.R")
```

# Data Exploration 

```{r Import Data}
diamonds <- fread("diamonds.csv", sep=",", header = T) # Load your data, diamonds.csv

diamonds$V1 <- NULL # Remove column 'V1' as it is similar to an ID variable - no additional meaning derived

# Rename columns for more precise names
colnames(diamonds)[5] <- "depth_ratio" # depth to depth_ratio
colnames(diamonds)[8] <- "length" # x to length
colnames(diamonds)[9] <- "width"  # y to width
colnames(diamonds)[10] <- "depth" # z to depth

# Set data to frame to rename 'cut' item ("Very Good")
#diamonds = as.data.frame(diamonds)

# Rename variable cut to fix spacing
#diamonds["cut"][diamonds["cut"] == 'Very Good'] <- 'Very_Good'

# Reset dataframe back to datatable for analysis
#diamonds = as.data.table(diamonds)
```

## Dimension Summary 

```{r Data Exploration}
dim(diamonds) # Dimensions of data
summary(diamonds) # Produce result summaries of all variables
str(diamonds) # Type of variables

# Number of unique values in each variable
sapply(diamonds, function(x) length(unique(x)))
```

## Missing Values

```{r Missing Values}
# Missing values analysis
gg_miss_var(diamonds) + ggtitle("Missing values")

# pairs(diamonds[, c(1, 5:10)])
```


```{r Variables check}
# carat no problems
unique(diamonds$cut) # Review unique values for cut
diamonds$cut <- as.factor(diamonds$cut) # Factor the cut to five levels 
diamonds$cut <- ordered(diamonds$cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal")) # Ordered from worst to best

unique(diamonds$color) # Review unique values for color
diamonds$color <- as.factor(diamonds$color) # Factor the color to seven levels 
diamonds$color <- ordered(diamonds$color, levels = c("J", "I", "H", "G", "F", "E", "D")) # Ordered from worst to best

unique(diamonds$clarity) # Review unique values for clarity
diamonds$clarity <- as.factor(diamonds$clarity) # Factor the clarity to eight levels 
diamonds$clarity <- ordered(diamonds$clarity, levels = c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF")) # Ordered from worst to best

# table is ok

# price is ok

# Remove values of 0 for for dimensions which includes zeros in length and width
nrow(diamonds[depth %in% 0,]) # Remove 20 rows due to depth = 0.0
diamonds <- diamonds[depth > 0, ] # Include only values with depth greater than zero

# Create formula to check the absolute value of length to width, comparison 
diamonds[, subtraction := abs(length - width)]
nrow(diamonds[subtraction>10,]) # Remove 2 rows due their extreme subtraction value (~59 and ~26)
diamonds <- diamonds[subtraction <= 10, ] # Include only values with subtraction less than ten

diamonds[, depth_check := round(100*(2*depth)/((length + width)), 1)]
diamonds[, diff := abs(depth_check-depth_ratio)]
# treshold at 0.3? anastasia
nrow(diamonds[diff > 0.3,]) # we remove 268 rows
diamonds <- diamonds[diff <= 0.3,]
# hist(diamonds[diff >= 0.4 & diff < 1, diff], breaks = 50)

# Removed created columns needed to clean the data
diamonds[, subtraction := NULL]
diamonds[, depth_check := NULL]
diamonds[, diff := NULL]
# Total rows remove: 275 observations
```


```{r ordering cols}
# Reorder data table to group like variable types 
diamonds <- diamonds[, c(7, 2:4, 1, 8:10, 5:6)]
```

```{r ggpairs, fig.width = 6, fig.height = 4}
# Used ggpairs to create a scatterplot matrix
# ggpairs(diamonds[, c(1, 5:10)], title = "Scatterplot Matrix",
#          proportions = "auto",
#          columnLabels = c("Price", "Carat", "Length", "Width", "Depth","Depth Ratio","Table"),
#          upper = list(continuous = wrap('cor',size = 3)),) + theme_light()
```

```{r Price by Carat and Clarity}
# Create plot that looks at carat and price
ggplot(aes(x = carat, y = price), data = diamonds) + geom_point(alpha = 0.5, size = 1, position = 'jitter',aes(color=clarity)) +
  scale_color_brewer(type = 'div', guide = guide_legend(title = 'Clarity', reverse = T,override.aes = list(alpha = 1, size = 2)))       + ggtitle('Price by Carat and Clarity')
```

```{r Depth Ratio vs Price}
plot(diamonds$depth_ratio,diamonds$price, col = "#F9D53E")
```

## Variable Visualisation

```{r Histograms,fig.width = 4, fig.height = 4}
ggplot(gather(data = diamonds[, c(1, 5:10)]), aes(value)) +
  geom_histogram(aes(y = after_stat(density)),
                 bins = 10,
                 color = "white",
                 fill = "#F9D53E") + # Creates bin sizing and sets the lines as white
  geom_density(alpha = .2, fill = "#F9D53E") +
  facet_wrap(~ key, scales = "free") + # Converting the graphs into panels
  ggtitle("Quantitative Variable Analysis") + # Title name
  ylab("Count") + xlab("Value") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines
```

```{r Correlation, fig.width = 4, fig.height = 4}
# Create heatmap to show variable correlation
# Round the correlation coefficient to two decimal places
cormat <- round(cor(diamonds[, c(1, 5:10)]), 2)

# Use correlation between variables as distance
reorder_cormat <- function(cormat){ 
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
return(cormat)
}

# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)

# Keeping only upper triangular matrix
# upper_tri returns TRUE/FALSE for each coordinate (TRUE -> part of upper triangle)
# multiplying will thus keep the upper triangle values and set the others to 0
cormat <- cormat*upper.tri(cormat, diag = TRUE)
# Values of the lower triangle (0) are replaced by NA
cormat[cormat == 0] <- NA

# Melt the correlation matrix
cormat <- reshape2::melt(cormat, na.rm = TRUE)

# Create a ggheatmap with multiple characteristics 
ggplot(cormat, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "#15DDD8", high = "#F9D53E", mid = "white",
                       midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  ggtitle("Correlation Heatmap") + # Title name
  theme_minimal() + # Minimal theme, keeps in the lines
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
  coord_fixed() +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2)

rm(cormat, reorder_cormat)
```

```{r Correlation Plot,fig.width = 5, fig.height = 5}
plotcorr(cor(diamonds[, -c(2:4)]), col = "#F9D53E",
         main = "Pearson correlation ellipses for numerical variables")
```

```{r train-valid-test}
# set seed for reproducing the partition
set.seed(111)

# generating training set index
train.index <- sample(c(1:nrow(diamonds)), 0.5*nrow(diamonds))
# generating validation set index taken from the complementary of training set
valid.index <- sample(setdiff(c(1:nrow(diamonds)), train.index), 0.3*nrow(diamonds))
# defining test set index as complementary of (train.index + valid.index)
test.index <- as.numeric(setdiff(row.names(diamonds), union(train.index, valid.index)))
# creating data tables Train, Valid and Test using the indexes
Train <- diamonds[train.index, ]
Valid <- diamonds[valid.index, ]
Test <- diamonds[test.index, ]
```

# Dimension Reduction Analysis

```{r VIF}
#diamonds_lm <- lm(price ~ carat + length + width + depth + depth_ratio + table, data = diamonds)
#diamonds_lm2 <- lm(price ~ carat + depth_ratio + table, data = diamonds)
#diamonds_lm3 <- lm(price ~ carat + depth + depth_ratio + table, data = diamonds)

#diamonds_vif <- vif(diamonds_lm)
#VIF(diamonds[, c(5:8)])
#diamonds_vif2 <- vif(diamonds_lm2)

#diamonds_vif3 <- vif(diamonds_lm3)


#summary(diamonds_vif)
```


# Variable Prediction and Model Performance Evaluation

## Linear Regression

```{r}

```

## $k$-NN

```{r}

```

## Regression Tree

```{r RT - Train, Valid, Test }
#Rename data specifically for regression trees
diamonds_tree <- diamonds

# Creating data tables Train, Valid and Test using the indexes for the regression tree section
Train <- diamonds[train.index, ]
Valid <- diamonds[valid.index, ]
Test <- diamonds[test.index, ]
```

```{r Price Correlation}
# Correlation between price and quantitative variables
price_correlation <- with(diamonds_tree,
     data.frame(cor_length_price = cor(length, price), cor_width_price = cor(width, price), cor_depth_price = cor(depth, price), cor_depth_ratio_price = cor(depth_ratio, price), cor_table_price2 = cor(table, price), cor_carat_price3 = cor(carat, price)))

# Transpose data and put into kable format
transpose <- t(sort(round(price_correlation,4),decreasing = FALSE))
kable_corr <- kable(transpose) %>% kable_classic() 
kable_corr
```

```{r Default Regression Tree}
# Use rpart() to run tree on continuous response 
RegressTree <- rpart(price ~ ., 
              data = Train, 
              method = "anova") 

# Generates a cost complexity parameter table that provides the complexity parameter value
#summary(RegressTree)

# Plots a regression tree
fancyRpartPlot(RegressTree, caption = NULL, main = "Regression Tree", palettes = "YlGnBu", digits = -3)

# Count number of leaves 
length(RegressTree$frame$var[RegressTree$frame$var == "<leaf>"]) 
```

```{r ktable Importance - RegressTree}
# kable and kable_styling as before
# We multiply by 100, divide by the sum and round the percentages to 2 decimals
kable_styling(kable(round(100*RegressTree$variable.importance / sum(RegressTree$variable.importance), 2), col.names = "Importance %"), full_width = FALSE)
```

```{r Error Details on Default Tree - Valid}
# Predict errors using accuracy() 
tree_train_error <- forecast::accuracy(predict(RegressTree, Valid), Valid$price)

kable(tree_train_error) %>% kable_classic() 
```

```{r R-adjusted on Default Tree - Train}
# Calculate R^2 
p <- predict(RegressTree, Train)
R_sq <- (cor(Train$price,p))^2
R_pct <- R_sq*100
R_pct
```

```{r Default Tree Predictions}
# Predict the diamond price with validation
pred_Diamond_test <- predict(RegressTree, newdata = Valid)

# Display first 14 observations
head(pred_Diamond_test,14)
```

```{r Default Regression Tree Minus Variables}
RegressTree2 <- rpart(price ~ length+width+depth+carat+cut+color+clarity, 
              data = Train, 
              method = "anova") 

# Generates a cost complexity parameter table that provides the complexity parameter value
#summary(RegressTree2)

# Plots a regression tree
fancyRpartPlot(RegressTree2, caption = NULL, main = "Exclusion Regression Tree", palettes = "YlGnBu", digits = -3)
```

```{r ktable Importance - RegressTree2}
# kable and kable_styling as before
# We multiply by 100, divide by the sum and round the percentages to 2 decimals
kable_styling(kable(round(100*RegressTree2$variable.importance / sum(RegressTree2$variable.importance), 2), col.names = "Importance %"), full_width = FALSE)
```

```{r Error Details on RegressTree2 - Valid}
# Predict errors using accuracy() 
tree_train_error2 <- forecast::accuracy(predict(RegressTree2, Valid), Valid$price)

kable(tree_train_error2) %>% kable_classic() 
```

```{r R-adjusted on RegressTree2}
# Calculate R^2 
p2 <- predict(RegressTree2, Train)
R_sq2 <- (cor(Train$price,p2))^2
R_pct2 <- R_sq2*100
R_pct2
```

```{r Default Tree Predictions2}
# Predict the diamond price with validation
pred_Diamond_test2 <- predict(RegressTree2, newdata = Valid)

# Display first 14 observations
head(pred_Diamond_test2,14)
```

```{r CP Value}
#Get the lowest CP value from CP table
min.xerror <- RegressTree2$cptable[which.min(RegressTree2$cptable[,"xerror"]),"CP"]

min.xerror
```
```{r Plot Cp}
#Plot the optimal Cp value
plotcp(RegressTree2)
```

```{r Pruned Regression Tree Minus Variables}
RegressTree_pruned <- prune(RegressTree2, cp = min.xerror) 

# Draw the prune tree
fancyRpartPlot(RegressTree_pruned, caption = NULL, main = "'Pruned' Regression Tree", palettes = "YlGnBu", digits = -3)
```

```{r Error Details on Prune_RegressTree2 - Valid}
# Predict errors using accuracy() 
tree_train_error3 <- forecast::accuracy(predict(RegressTree_pruned, Valid), Valid$price)

kable(tree_train_error3) %>% kable_classic() 
```


```{r Boosted Tree 10}
# Boosted tree
set.seed(111)
tree_boost10 <- gbm(price ~., data = Train, distribution = "gaussian", n.trees = 10, interaction.depth = 6, shrinkage = 0.01)

# Interaction Depth specifies the maximum depth of each tree( i.e. highest level of variable interactions allowed while training the model). Default is 1.
# Shrinkage is considered as the learning rate. It is used for reducing, or shrinking, the impact of each additional fitted base-learner (tree).  It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. Default is 0.1
#n.trees: Integer specifying the total number of trees to fit. This is equivalent to the number of iterations and the number of basis functions in the additive expansion. Default is 100.


tree_boost10

vip::vip(tree_boost10, aesthetics = list(fill = "#F9D53E")) +
  ggtitle("Boosted Tree Variable Importance") + # Title name
  xlab("Variable") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines
```

```{r Error Details on tree_boost10 - Valid}
# Predict errors using accuracy() 
tree_boost10_train <- forecast::accuracy(predict(tree_boost10, Valid), Valid$price)

kable(tree_boost10_train) %>% kable_classic() 
```

```{r Boosted Tree 30}
# Boosted tree
set.seed(111)
tree_boost30 <- gbm(price ~., data = Train, distribution = "gaussian", n.trees = 30)
tree_boost30


vip::vip(tree_boost30, aesthetics = list(fill = "#F9D53E")) +
  ggtitle("Boosted Tree Variable Importance") + # Title name
  xlab("Variable") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines
```

```{r Error Details on tree_boost30}
# Predict errors using accuracy() 
tree_boost30_train <- forecast::accuracy(predict(tree_boost30, Train), Train$price)

kable(tree_boost30_train) %>% kable_classic() 
```

```{r Boosted Tree 100}
# Boosted tree
set.seed(111)
tree_boost100 <- gbm(price ~., data = Train, distribution = "gaussian", cv.folds = 3)

# cv.folds: Number of cross-validation folds to perform. If cv.folds>1 then gbm, in addition to the usual fit, will perform a cross-validation, calculate an estimate of generalization error returned in cv.error.

tree_boost100

vip::vip(tree_boost100, aesthetics = list(fill = "#F9D53E")) +
  ggtitle("Boosted Tree Variable Importance") + # Title name
  xlab("Variable") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines
```

```{r Error Details on tree_boost100}
# Predict errors using accuracy() 
tree_boost100_train <- forecast::accuracy(predict(tree_boost100, Valid), Valid$price)

kable(tree_boost100_train) %>% kable_classic()
```

```{r Boosted Tree 100 with oob}
ntree_opt_oob <- gbm.perf(tree_boost100, method = "OOB")
ntree_opt_oob

# Indicate the method used to estimate the optimal number of boosting iterations. method = "OOB" computes the out-of-bag estimate and method = "test" uses the test (or validation) dataset to compute an out-of-sample estimate. method = "cv" extracts the optimal number of iterations using cross-validation if gbm was called with cv.folds > 1.
```

```{r Predict Boosting}
pred_boost <- predict.gbm(tree_boost100, newdata = Valid)
```

```{r Bagged Tree}
# Bagged tree
set.seed(111)
tree_bagging <- bagging(price ~., data = Train, coob = TRUE)
tree_bagging
```

```{r VI for Bagging}
# Variable importance for bagging
pred_imp <- as.data.frame(varImp(tree_bagging))
pred_imp <- data.frame(overall = pred_imp$Overall,
           names   = rownames(pred_imp))

pred_imp[order(pred_imp$overall,decreasing = T),]
```

```{r Plot IV, fig.width = 4, fig.height = 3}
pred_imp <- varImp(tree_bagging)

#sort variable importance descending
VI_plot <- pred_imp[order(pred_imp$Overall, decreasing=FALSE),]

#visualize variable importance with horizontal bar plot
barplot(sort(pred_imp$Overall, decreasing = FALSE),
        names.arg=rownames(pred_imp),
        horiz=TRUE,
        col="#F9D53E", las = 1)

# Add a main title and bottom and left axis labels
title("Variable Importance ", xlab="Variables")
```

```{r Errors Bagging}
tree_bagging_train <- forecast::accuracy(predict(tree_bagging, Valid), Valid$price)

kable(tree_bagging_train) %>% kable_classic()
```

```{r}
pred_bagged <- predict(tree_bagging, newdata = Valid)

head(pred_bagged,14)
```


```{r randomForest}
options(scipen = 9999)
set.seed(111)
rdf_model <- randomForest(price~ ., ntree= 60, data = Train)
plot(rdf_model)
```

```{r Optimal Random Trees}
# Number of trees with lowest MSE
which.min(rdf_model$mse)
```

```{r}
df2=df[order(df$depth),]
df2$name=factor(df2$name,levels=df2$name)
ggplot(df2,aes(x=factor(name),y=depth)) + geom_bar(stat='identity') + coord_flip() + labs(y='depth',x='species')

df2=df[order(df$depth),]
df2$name=factor(df2$name,levels=df2$name)
ggplot(df2,aes(x=factor(name),y=depth)) + geom_bar(stat='identity') + coord_flip() + labs(y='depth',x='species')

var_importance <- data_frame(variable=setdiff(colnames(Train), "price"), importance=as.vector(importance(rdf_model)))

var_importance <- arrange(var_importance, desc(importance))

var_importance$variable <- factor(var_importance$variable, levels=var_importance$variable)

var_importance <- var_importance[order(var_importance$variable),]

p <- ggplot(var_importance, aes(x=variable, weight=importance))
p <- p + geom_bar(fill='#F9D53E', color='#F9D53E') + ggtitle("Random Forest Variable Importance")
p <- p + xlab("Variable") + ylab("Importance")
p <- p + coord_flip()
p + theme_classic()
```

```{r Plot Random IV, fig.width = 4, fig.height = 4}
pred_imp_rdf <- varImp(rdf_model)

#sort variable importance descending
VI_plot <- pred_imp_rdf[order(pred_imp_rdf$Overall, decreasing=TRUE),]

#visualize variable importance with horizontal bar plot
barplot(sort(pred_imp_rdf$Overall, decreasing = FALSE),
        names.arg=rownames(pred_imp_rdf),
        horiz=TRUE,
        col="#F9D53E", las = 1)

# Add a main title and bottom and left axis labels
title("Variable Importance ", xlab="Variables")
```

```{r}
rdf_model_train <- forecast::accuracy(predict(rdf_model, Valid), Valid$price)

kable(rdf_model_train) %>% kable_classic()
```

## NeuralNetworks

```{r}

```

## Ensembles

```{r}

```

# Model Performance Summary

# Conclusions

