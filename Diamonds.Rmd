---
title: "Diamonds"
author: "Francisco Arrieta, Emily Schmidt and Lucia Camenisch"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true             # creating a table of contents (toc)
    toc_float: 
      collapsed: false    # toc does not collapse and is shown as a sidebar (toc_float)
    number_sections: true # document sections are numbered
    theme: cosmo
---

```{=html}
<style>
body{
  color: #2F91AE;
  background-color: #F2F2F2;
}
pre{
  background-color: #96EAE3;
}
pre:not([class]){
  background-color: #15DDD8;
}
.toc-content{
  padding-left: 10px;
  padding-right: 10px;
}
.col-sm-8 {
  width: 75%;
}
code {
  color: #333333;
  background-color: #96EAE3;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.align = "center")
```

```{r Colors, include=FALSE}
#Aqua =         "#15DDD8"
#Dark Blue =    "#2F91AE"
#Yellow =       "#F9D53E"
#Light Gray =   "#F2F2F2"
#Light Aqua =   "#96EAE3"
```

```{r Libraries}
library(data.table)     #for reading data.tables
library(ggplot2)        #for making graphs
library(tidyr)          #for changing the shape and hierarchy of a data set
library(ellipse)        #for mapping correlation
library(e1071)          #for skewness
library(caret)          #for preProcess() and accuracy()
library(fastDummies)    #for creating dummies
library(forecast)       # for accuracy() measures
library(kableExtra)     #for more elaborate tables
# library(GGally)         #for making graphs
# library(dplyr)          #for data manipulation
# library(DataExplorer)   #for graphing missing value percentages
# library(car)            #for statistic functions


source("VIF.R")
source("ProcStep.R")
source("GlobalCrit.R")
options(scipen = 999)
```

# Data Exploration 

```{r Import Data}
diamonds <- fread("diamonds.csv") # Load your data, diamonds.csv

diamonds$V1 <- NULL # Remove column 'V1' as it is similar to an ID variable - no additional meaning derived

# Rename columns for more precise names
colnames(diamonds)[5] <- "depth_ratio" # depth to depth_ratio
colnames(diamonds)[8] <- "length" # x to length
colnames(diamonds)[9] <- "width"  # y to width
colnames(diamonds)[10] <- "depth" # z to depth
```


```{r categorical-as-factors}
# Review unique values for cut
unique(diamonds$cut)
# Factor the cut to five levels
diamonds$cut <- as.factor(diamonds$cut)
# Ordered from worst to best
diamonds$cut <- ordered(diamonds$cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))

# Review unique values for color
unique(diamonds$color)
# Factor the color to seven levels 
diamonds$color <- as.factor(diamonds$color) 
# Ordered from worst to best
diamonds$color <- ordered(diamonds$color, levels = c("J", "I", "H", "G", "F", "E", "D"))

# Review unique values for clarity
unique(diamonds$clarity)
# Factor the clarity to eight levels 
diamonds$clarity <- as.factor(diamonds$clarity)
# Ordered from worst to best
diamonds$clarity <- ordered(diamonds$clarity,
                            levels = c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
```


```{r removing-rows}
# Remove values of 0 for for dimensions which includes zeros in length and width
nrow(diamonds[depth %in% 0,])        # Remove 20 rows due to depth = 0.0
diamonds <- diamonds[depth > 0, ]    # Include only values with depth greater than zero

# Create formula to check the absolute value of length to width, comparison 
diamonds[, subtraction := abs(length - width)]
# Remove 2 rows due their extreme subtraction value (~59 and ~26)
nrow(diamonds[subtraction>10,])
# Include only values with subtraction less than ten
diamonds <- diamonds[subtraction <= 10, ]

diamonds[, depth_check := round(100*(2*depth)/((length + width)), 1)]
diamonds[, diff := abs(depth_check-depth_ratio)]
# treshold at 0.3? anastasia
nrow(diamonds[diff > 0.3,]) # we remove 253 rows
diamonds <- diamonds[diff <= 0.3,]
# hist(diamonds[diff >= 0.4 & diff < 1, diff], breaks = 50)

# Removed created columns needed to clean the data
diamonds[, subtraction := NULL]
diamonds[, depth_check := NULL]
diamonds[, diff := NULL]
# Total rows removed: 275 observations
```


```{r ordering cols}
# Reorder data table to group like variable types 
diamonds <- diamonds[, c(7, 2:4, 1, 8:10, 5:6)]
```



```{r Histograms}
ggplot(gather(data = diamonds[, c(1, 5:10)]), aes(value)) +
  geom_histogram(aes(y = after_stat(density)),
#                 bins = 10,
                 color = "white",
                 fill = "#F9D53E") + # Creates bin sizing with colors
  geom_density(alpha = .2, fill = "#F9D53E") +
  facet_wrap(~ key, scales = "free") + # Converting the graphs into panels
  ggtitle("Quantitative Variable Analysis") + # Title name
  ylab("Count") + xlab("Value") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines
```

```{r Correlation}
# Create heatmap to show variable correlation
# Round the correlation coefficient to two decimal places
cormat <- round(cor(diamonds[, c(1, 5:10)]), 2)

# Use correlation between variables as distance
reorder_cormat <- function(cormat){ 
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
return(cormat)
}

# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)

# Keeping only upper triangular matrix
# upper_tri returns TRUE/FALSE for each coordinate (TRUE -> part of upper triangle)
# multiplying will thus keep the upper triangle values and set the others to 0
cormat <- cormat*upper.tri(cormat, diag = TRUE)
# Values of the lower triangle (0) are replaced by NA
cormat[cormat == 0] <- NA

# Melt the correlation matrix
cormat <- reshape2::melt(cormat, na.rm = TRUE)

# Create a ggheatmap with multiple characteristics 
ggplot(cormat, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "#15DDD8", high = "#F9D53E", mid = "white",
                       midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  ggtitle("Correlation Heatmap") + # Title name
  theme_minimal() + # Minimal theme, keeps in the lines
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
  coord_fixed() +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2)

rm(cormat, reorder_cormat)
```

```{r Correlation Plot}
plotcorr(cor(diamonds[, -c(2:4)]), col = "#F9D53E",
         main = "Pearson correlation ellipses for numerical variables")
```

```{r train-valid-test-indexes}
# set seed for reproducing the partition
set.seed(111)

# generating training set index
train.index <- sample(c(1:nrow(diamonds)), 0.5*nrow(diamonds))
# generating validation set index taken from the complementary of training set
valid.index <- sample(setdiff(c(1:nrow(diamonds)), train.index), 0.3*nrow(diamonds))
# defining test set index as complementary of (train.index + valid.index)
test.index <- as.integer(setdiff(row.names(diamonds), union(train.index, valid.index)))
```


## Linear Regression

We begin by performing linear regressions on our data. As we saw during the exploration phase, some predictors (`carat`, `length`, `width` and `depth`) are highly correlated. Therefore, multicollinearity might be an issue.

We use the Generalized Variation Inflation Factors (GVIF) to measure the multicollinearity level of our data. This generalized version of the VIF allows us to take into account numerical and categorical predictors together. The GVIF clearly confirms that there is an issue, as `length`, `width` and `depth` all have coefficients above 1000. `carat` and `depth_ratio` also have high values above 25, but they aren't as high extreme as the other three.

After removing `length`, `width` and `depth`, the GVIF coefficients of the remaining predictors are all under 2, which indicates the multicollinearity problem is solved. We display correlation ellipses of numerical variables without `length`, `width` and `depth`.

```{r LR data partition}
# creating training and validation sets
Train_lr <- diamonds[train.index, ]
Valid_lr <- diamonds[valid.index, ]
```

```{r vif}
# using the VIF function from statistical modelling to check multicollinearity of predictors
VIF(y = diamonds$price, matx = diamonds[, -c(1)])
# removing length, width and depth and computing VIF without them
VIF(y = diamonds$price, matx = diamonds[, -c(1, 6, 7, 8)])

# plotting correlation ellipses of numerical variables with length, width and depth removed
plotcorr(cor(diamonds[, -c(2:4, 6:8)]), col = "#D8B365",
         main = "Pearson correlation ellipses for numerical variables")
```

The only high correlation left is between `carat` and `price`, indicating that `carat` will surely be an important predictor for determining `price`.

Thus, we will perform linear regressions on two different models:

1.  `LM_complete` which contains all predictors
2.  `LM_minus_corr` which has `length`, `width` and `depth` removed.

These two models will serve as basis for variable selection procedures later.

However, before starting to build our models, we also need to account for skewed variables. Linear regression might perform worse when dealing with skewed variables and it is common to use transformations such as a logarithm or a $n$th root to make variable more symmetrical.

We use an estimator of skewness called \$b_1\$, whose definition can be found [here](https://en.wikipedia.org/wiki/Skewness#Sample_skewness).

The value of $b_1$ is interpreted as follows:

-   $0 \leq |b_1| < 0.5$: variable is symmetrical;
-   $0.5 \leq |b_1| < 1$: variable is moderately skewed;
-   $|b_1| \geq 1$: variable is highly skewed.

We compute $b_1$ on our numerical variables and get the following results.

```{r skewness}
# applying the skewness() function of every numerical variable from our training set
kable(sapply(Train_lr[, c(1, 5:10)], skewness), col.names = "$b_1$")

# logarithmic transformation on price, carat and table
Train_lr$price <- log(Train_lr$price)
Train_lr$carat <- log(Train_lr$carat)
Train_lr$table <- log(Train_lr$table)

# recomputing the skewness of numerical variables to see the improvement
sapply(Train_lr[, c(1, 5:10)], skewness)

# transforming in the validation set as well
Valid_lr$price <- log(Valid_lr$price)
Valid_lr$carat <- log(Valid_lr$carat)
Valid_lr$table <- log(Valid_lr$table)
```

`price` and `carat` are highly skewed and `table` is moderately skewed.

We apply a logarithmic transformation to all three variables. The improvement can also be seen in the histograms, as they look more symmetrical now.

```{r hist-unskewed}
# computing the histograms of numerical variables now that they are unskewed

ggplot(gather(data = Train_lr[, c(1, 5:10)]), aes(value)) + # numerical vars of training set
  geom_histogram(aes(y = after_stat(density)),              # making histograms with color params
                 color = "white",
                 fill = "#F9D53E") +             
  geom_density(alpha = .2, fill = "#F9D53E") +              # making density lines
  facet_wrap(~ key, scales = "free") +                      # multiple plots with facet
  labs(title = "Histograms of numerical variables",         # labels of the plot
       x = "Count",
       y = "Value") +
  theme_classic()                                           # aesthetic theme
```

We also standardize numerical variables by subtracting their mean and dividing by their standard deviation. This makes the comparison of $\beta$ coefficients between variables easier.

```{r normalizing}
# we compute the the mean and std values based on training data (for numerical variables)
norm.values <- preProcess(Train_lr[, c(1, 5:10)], method=c("center", "scale"))

# we standardize the training and validation data
Train_lr[, c(1, 5:10)] <- predict(norm.values, Train_lr[, c(1, 5:10)])
Valid_lr[, c(1, 5:10)] <- predict(norm.values, Valid_lr[, c(1, 5:10)])
```

The data is now ready for our linear models. For both `LM_complete` and `LM_minus_corr`, we perform the following linear regressions:

1. Linear regression on the whole model
2. Forward selection on the model (iterative method)
3. Backward selection on the model (iterative method)
4. Stepwise selection on the model (iterative method)
5. Mallows's $C_p$ and AIC selection on the model (global method)


```{r LM-complete}
# we compute the linear model using all predictors and display its summary
LM_complete = lm(price ~. , data = Train_lr)
summary(LM_complete)
```

```{r iterative-search-complete}
# we use iterative search algorithms on the complete model: forward, backward and stepwise
# we display summaries of the three models obtained with iterative methods

LM_forward_complete = step(LM_complete, direction = "forward")
summary(LM_forward_complete)

LM_backward_complete = step(LM_complete, direction = "backward")
summary(LM_backward_complete)

LM_stepwise_complete = step(LM_complete, direction = "both")
summary(LM_stepwise_complete)
```

```{r LM_CpAIC_complete}
# we use the GlobalCrit function from statistical modelling to reduce the number of predictors
# using global criterions: Mallows's Cp and AIC
GlobalCrit(LM_complete)

# we compute the linear model obtained with global methods and display its summary
LM_CpAIC_complete = lm(price ~ . , data = Train_lr[, c(1:6, 8, 9)])
summary(LM_CpAIC_complete)
```




```{r LM-minus-corr}
# we define a training set without the correlated predictors (length, width, depth)
Train_minus_corr <- Train_lr[, -c(6:8)]

# we compute the linear model without correlated predictors and display its summary
LM_minus_corr = lm(price ~ ., data = Train_minus_corr)
summary(LM_minus_corr)
```

```{r iterative-search-minus-corr}
# we use iterative search algorithms on the model without corr: forward, backward and stepwise
# we display summaries of the three models obtained with iterative methods

LM_backward_minus_corr = step(LM_minus_corr, direction = "backward")
summary(LM_backward_minus_corr)

LM_forward_minus_corr = step(LM_minus_corr, direction = "forward")
summary(LM_forward_minus_corr)

LM_stepwise_minus_corr = step(LM_minus_corr, direction = "both")
summary(LM_stepwise_minus_corr)
```

```{r LM_CpAIC_minus_corr}
# we use the GlobalCrit function from statistical modelling to reduce the number of predictors
# using global criterions: Mallows's Cp and AIC
GlobalCrit(LM_minus_corr)

# we compute the linear model obtained with global methods and display its summary
LM_CpAIC_minus_corr = lm(price ~ . , data = Train_lr[, c(1:5, 9)])
summary(LM_CpAIC_minus_corr)
```

For each of these models, we summarise which variables are used as predictors in the following table.

```{r predictors-table}
# we display a table of the predictors used in each model using kable()
# kable_styling() controls the parameter fullwidth for the total width of table

kable_styling(
  kable(
    # the data to display in the table: model names and crosses for presence of predictor
    data.table(Model = c("LM_complete", "LM_forward_complete", "LM_backward_complete",
                       "LM_stepwise_complete", "LM_CpAIC_complete",
                       "LM_minus_corr", "LM_forward_minus_corr", "LM_backward_minus_corr",
                       "LM_stepwise_minus_corr", "LM_CpAIC_minus_corr"),
             Cut           = c("X","X","X","X","X","X","X","X","X","X"),
             Color         = c("X","X","X","X","X","X","X","X","X","X"),
             Clarity       = c("X","X","X","X","X","X","X","X","X","X"),
             Carat         = c("X","X","X","X","X","X","X","X","X","X"),
             Length        = c("X","X","X","X","X"," "," "," "," "," "),
             Width         = c("X","X"," "," "," "," "," "," "," "," "),
             Depth         = c("X","X","X","X","X"," "," "," "," "," "),
             `Depth Ratio` = c("X","X","X","X","X","X","X","X","X","X"),
             Table         = c("X","X"," "," "," ","X","X"," "," "," ")
             ),
    align = 'lccccccccc',                               # alignment of each column
    caption = "Predictors used in each linear model"),  # caption of the table
  full_width = FALSE)                                   # table isn't full width of page
```

For both basis models, forward selection doesn't discard any variables, whereas backward, stepwise and global selections all choose the same model with less variables than initially.

Thus, we have four distinct models in total. We assess the predictive performance of these four models on our validation set by computing the five accuracy measures seen during the course.

Let us recall the definitions and meaning of these measures (Data Mining for Business Analytics - Concepts, Techniques, and Applications in R, chapter 5.2, page 119). We denote the residuals by $r = y - \hat y$.

1.  **ME** (Mean Error) gives an indication of whether the predictions are on average over- or under-predicting the outcome variable. $$ME = \dfrac{1}{n} \sum_{i=1}^{n}r_i$$
2.  **RMSE** (Root Mean Squared Error) is similar to the standard error of estimate in linear regression, except that it is computed on the validation data rather than on the training data. $$RMSE =  \sqrt{\dfrac{1}{n}\sum_{i=1}^{n}r_i^2}$$
3.  **MAE** (Mean Absolute Error) gives the magnitude of the average absolute error. $$MAE = \dfrac{1}{n} \sum_{i=1}^{n}|r_i|$$
4.  **MPE** (Mean Percentage Error) gives the percentage score of how predictions deviate from the actual values (on average), taking into account the direction of the error. $$MPE = 100 \cdot \dfrac{1}{n} \sum_{i=1}^{n} \dfrac{r_i}{y_i}$$
5.  **MAPE** (Mean Absolute Percentage Error) gives a percentage score of how predictions deviate (on average) from the actual values. $$MAPE = 100 \cdot \dfrac{1}{n} \sum_{i=1}^{n} \left| \dfrac{r_i}{y_i}\right|$$

```{r removing-models}
# we remove redundant models
rm(LM_forward_complete, LM_backward_complete, LM_stepwise_complete, LM_forward_minus_corr, LM_backward_minus_corr, LM_stepwise_minus_corr)
```


```{r predictions-table}
# predicting prices of validation set on the validation data

# we create a data table of predictions which contains predictions for the four models
# predictions are computed on validation set using predict()
LM_Predictions =
  data.table(
    LM_complete_pred = predict(object = LM_complete, newdata = Valid_lr),
    LM_CpAIC_complete_pred = predict(object = LM_CpAIC_complete, newdata = Valid_lr),
    LM_minus_corr_pred = predict(object = LM_minus_corr, newdata = Valid_lr),
    LM_CpAIC_minus_corr_pred = predict(object = LM_CpAIC_minus_corr, newdata = Valid_lr)
    )
```


```{r rescaling-predictions}
# we have to scale back the price, to do so we fetch the mean and std value from norm.values

# we display all means and stds
norm.values$mean
norm.values$std

# fetching for price
mean_price = norm.values$mean[1]
std_price = norm.values$std[1]

# scaling back (Y*mu + sigma), then exp() (we had transformed price with a log for skewness)
LM_Predictions = LM_Predictions*std_price + mean_price
LM_Predictions = exp(LM_Predictions)
```


```{r real-prices}
# taking real prices of validation data from diamonds (which has not been touched -> original scale)
LM_Predictions[, real_prices := diamonds[valid.index, price]]
```

The accuracy measures for our four models are summarized below. In order to give meaningful results, the outcome variable `price` has been rescaled to its original scale and retransformed by taking the exponential (to cancel out the logarithm). Thus, the ME, RMSE and MAE can be interpreted in the price currency, dollars.

```{r models-accuracy-measures}
# we compute accuracy measures for each model using accuracy() from the forecast package
# we convert each accuracy to a table (in order to put them together afterwards)

Acc1 = accuracy(object = LM_Predictions$LM_complete_pred, x = LM_Predictions$real_prices)
Acc1 = as.data.table(Acc1)
Acc2 = accuracy(object = LM_Predictions$LM_CpAIC_complete_pred, x = LM_Predictions$real_prices)
Acc2 = as.data.table(Acc2)
Acc3 = accuracy(object = LM_Predictions$LM_minus_corr_pred, x = LM_Predictions$real_prices)
Acc3 = as.data.table(Acc3)
Acc4 = accuracy(object = LM_Predictions$LM_CpAIC_minus_corr_pred, x = LM_Predictions$real_prices)
Acc4 = as.data.table(Acc4)

# we create a list of accuracy measures
Accs = list(Acc1, Acc2, Acc3, Acc4)
# we use rbindlist() from the data.table package to stack the four tables on top of eachother
Accs = rbindlist(Accs)
# now that Accs is created we can remove the four tables
rm(Acc1, Acc2, Acc3, Acc4)
# we add a column to Accs with the model names
Accs[, Model := c("LM_complete", "LM_CpAIC_complete", "LM_minus_corr", "LM_CpAIC_minus_corr")]
# we put the Model column first
Accs <- Accs[, c(6, 1:5)]

# displaying Accs as a table with kable() and adding a caption
# kable_styling controls width of table
kable_styling(kable(Accs, caption = "Accuracy measures of linear models"), full_width = FALSE)
```


The mean error is bigger in the models without multicollinearity issues, but their RMSE is smaller. Since the mean error is positive in all four models, we are under-predicting the price of diamonds by 36 or 50 dollars on average depending on the model.

An increase of 14$ in the mean error is a good trade-off to reducing the RMSE, which indicates how much the predictions will fluctuate from real values. The other three measures are quite close in all four models.

Considering the parsimony principle and the RMSE, the best choice is our fourth linear model, which contains `cut`, `color`, `clarity`, `carat` and `depth_ratio`.

Overall, the RMSE for linear regression remains quite high and as we will see in the following chapters, some models will achieve much better predictive performances.
