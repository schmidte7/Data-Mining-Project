---
title: "Diamonds"
author: "Francisco Arrieta, Emily Schmidt and Lucia Camenisch"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true             # creating a table of contents (toc)
    toc_float: 
      collapsed: false    # toc does not collapse and is shown as a sidebar (toc_float)
    number_sections: true # document sections are numbered
    theme: cosmo
---

```{=html}
<style>
body{
  color: #2F91AE;
  background-color: #F2F2F2;
}
pre{
  background-color: #96EAE3;
}
pre:not([class]){
  background-color: #15DDD8;
}
.toc-content{
  padding-left: 10px;
  padding-right: 10px;
}
.col-sm-8 {
  width: 75%;
}
code {
  color: #333333;
  background-color: #96EAE3;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.align = "center")
```

```{r Colors, include=FALSE}
#Aqua =         "#15DDD8"
#Dark Blue =    "#2F91AE"
#Yellow =       "#F9D53E"
#Light Gray =   "#F2F2F2"
#Light Aqua =   "#96EAE3"
```

```{r Libraries}
library(data.table)     #for reading data.tables
library(kableExtra)     #for more elaborate tables
library(ggplot2)        #for making graphs
library(GGally)         #for making graphs
library(dplyr)          #for data manipulation
library(tidyr)          #for changing the shape and hierarchy of a data set
library(DataExplorer)   #for graphing missing value percentages
library(car)            #for statistic functions
library(ellipse)        #for mapping correlation
library(naniar)         #for missing values 

```

# Data Exploration 

```{r Import Data}
diamonds <- fread("diamonds.csv", sep=",", header = T) # Load your data, diamonds.csv

diamonds$V1 <- NULL # Remove column 'V1' as it is similar to an ID variable - no additional meaning derived

# Rename columns for more precise names
colnames(diamonds)[5] <- "depth_ratio" # depth to depth_ratio
colnames(diamonds)[8] <- "length" # x to length
colnames(diamonds)[9] <- "width"  # y to width
colnames(diamonds)[10] <- "depth" # z to depth

# Set data to frame to rename 'cut' item ("Very Good")
#diamonds = as.data.frame(diamonds)

# Rename variable cut to fix spacing
#diamonds["cut"][diamonds["cut"] == 'Very Good'] <- 'Very_Good'

# Reset dataframe back to datatable for analysis
#diamonds = as.data.table(diamonds)
```

## Dimension Summary 

```{r Data Exploration}
dim(diamonds) # Dimensions of data
summary(diamonds) # Produce result summaries of all variables
str(diamonds) # Type of variables

# Number of unique values in each variable
sapply(diamonds, function(x) length(unique(x)))
```

## Missing Values

```{r Missing Values}
# Missing values analysis
gg_miss_var(diamonds) + ggtitle("Missing values")

# pairs(diamonds[, c(1, 5:10)])
```


```{r Variables check}
# carat no problems
unique(diamonds$cut) # Review unique values for cut
diamonds$cut <- as.factor(diamonds$cut) # Factor the cut to five levels 
diamonds$cut <- ordered(diamonds$cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal")) # Ordered from worst to best

unique(diamonds$color) # Review unique values for color
diamonds$color <- as.factor(diamonds$color) # Factor the color to seven levels 
diamonds$color <- ordered(diamonds$color, levels = c("J", "I", "H", "G", "F", "E", "D")) # Ordered from worst to best

unique(diamonds$clarity) # Review unique values for clarity
diamonds$clarity <- as.factor(diamonds$clarity) # Factor the clarity to eight levels 
diamonds$clarity <- ordered(diamonds$clarity, levels = c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF")) # Ordered from worst to best

# table is ok

# price is ok

# Remove values of 0 for for dimensions which includes zeros in length and width
nrow(diamonds[depth %in% 0,]) # Remove 20 rows due to depth = 0.0
diamonds <- diamonds[depth > 0, ] # Include only values with depth greater than zero

# Create formula to check the absolute value of length to width, comparison 
diamonds[, subtraction := abs(length - width)]
nrow(diamonds[subtraction>10,]) # Remove 2 rows due their extreme subtraction value (~59 and ~26)
diamonds <- diamonds[subtraction <= 10, ] # Include only values with subtraction less than ten

diamonds[, depth_check := round(100*(2*depth)/((length + width)), 1)]
diamonds[, diff := abs(depth_check-depth_ratio)]
# treshold at 0.3? anastasia
nrow(diamonds[diff > 0.3,]) # we remove 268 rows
diamonds <- diamonds[diff <= 0.3,]
# hist(diamonds[diff >= 0.4 & diff < 1, diff], breaks = 50)

# Removed created columns needed to clean the data
diamonds[, subtraction := NULL]
diamonds[, depth_check := NULL]
diamonds[, diff := NULL]
# Total rows remove: 275 observations
```


```{r ordering cols}
# Reorder data table to group like variable types 
diamonds <- diamonds[, c(7, 2:4, 1, 8:10, 5:6)]
```

```{r ggpairs, fig.width = 6, fig.height = 4}
# Used ggpairs to create a scatterplot matrix
# ggpairs(diamonds[, c(1, 5:10)], title = "Scatterplot Matrix",
#          proportions = "auto",
#          columnLabels = c("Price", "Carat", "Length", "Width", "Depth","Depth Ratio","Table"),
#          upper = list(continuous = wrap('cor',size = 3)),) + theme_light()
```

```{r Price by Carat and Clarity}
# Create plot that looks at carat and price
ggplot(aes(x = carat, y = price), data = diamonds) + geom_point(alpha = 0.5, size = 1, position = 'jitter',aes(color=clarity)) +
  scale_color_brewer(type = 'div', guide = guide_legend(title = 'Clarity', reverse = T,override.aes = list(alpha = 1, size = 2)))       + ggtitle('Price by Carat and Clarity')
```

```{r Price Correlation}
# Correlation between price and quantitative variables
price_correlation <- with(diamonds,
     data.frame(cor_length_price = cor(length, price), cor_width_price = cor(width, price), cor_depth_price = cor(depth, price), cor_depth_ratio_price = cor(depth_ratio, price), cor_table_price2 = cor(table, price), cor_carat_price3 = cor(carat, price)))

# Transpose data and put into kable format
transpose <- t(sort(round(price_correlation,4),decreasing = FALSE))
kable_corr <- kable(transpose) %>% kable_classic() 
kable_corr
```

```{r Depth Ratio vs Price}
plot(diamonds$depth_ratio,diamonds$price, col = "#F9D53E")
```

## Variable Visualisation

```{r Histograms,fig.width = 4, fig.height = 4}
ggplot(gather(data = diamonds[, c(1, 5:10)]), aes(value)) +
  geom_histogram(aes(y = after_stat(density)),
                 bins = 10,
                 color = "white",
                 fill = "#F9D53E") + # Creates bin sizing and sets the lines as white
  geom_density(alpha = .2, fill = "#F9D53E") +
  facet_wrap(~ key, scales = "free") + # Converting the graphs into panels
  ggtitle("Quantitative Variable Analysis") + # Title name
  ylab("Count") + xlab("Value") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines
```

```{r Correlation, fig.width = 4, fig.height = 4}
# Create heatmap to show variable correlation
# Round the correlation coefficient to two decimal places
cormat <- round(cor(diamonds[, c(1, 5:10)]), 2)

# Use correlation between variables as distance
reorder_cormat <- function(cormat){ 
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
return(cormat)
}

# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)

# Keeping only upper triangular matrix
# upper_tri returns TRUE/FALSE for each coordinate (TRUE -> part of upper triangle)
# multiplying will thus keep the upper triangle values and set the others to 0
cormat <- cormat*upper.tri(cormat, diag = TRUE)
# Values of the lower triangle (0) are replaced by NA
cormat[cormat == 0] <- NA

# Melt the correlation matrix
cormat <- reshape2::melt(cormat, na.rm = TRUE)

# Create a ggheatmap with multiple characteristics 
ggplot(cormat, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "#15DDD8", high = "#F9D53E", mid = "white",
                       midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  ggtitle("Correlation Heatmap") + # Title name
  theme_minimal() + # Minimal theme, keeps in the lines
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
  coord_fixed() +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2)

rm(cormat, reorder_cormat)
```

```{r Correlation Plot,fig.width = 5, fig.height = 5}
plotcorr(cor(diamonds[, -c(2:4)]), col = "#F9D53E",
         main = "Pearson correlation ellipses for numerical variables")
```

```{r train-valid-test}
# set seed for reproducing the partition
set.seed(111)

# generating training set index
train.index <- sample(c(1:nrow(diamonds)), 0.5*nrow(diamonds))
# generating validation set index taken from the complementary of training set
valid.index <- sample(setdiff(c(1:nrow(diamonds)), train.index), 0.3*nrow(diamonds))
# defining test set index as complementary of (train.index + valid.index)
test.index <- as.numeric(setdiff(row.names(diamonds), union(train.index, valid.index)))
# creating data tables Train, Valid and Test using the indexes
Train <- diamonds[train.index, ]
Valid <- diamonds[valid.index, ]
Test <- diamonds[test.index, ]
```

# Dimension Reduction Analysis

```{r VIF}
#diamonds_lm <- lm(price ~ carat + length + width + depth + depth_ratio + table, data = diamonds)
#diamonds_lm2 <- lm(price ~ carat + depth_ratio + table, data = diamonds)
#diamonds_lm3 <- lm(price ~ carat + depth + depth_ratio + table, data = diamonds)

#diamonds_vif <- vif(diamonds_lm)
#VIF(diamonds[, c(5:8)])
#diamonds_vif2 <- vif(diamonds_lm2)

#diamonds_vif3 <- vif(diamonds_lm3)


#summary(diamonds_vif)
```


# Variable Prediction and Model Performance Evaluation

## Linear Regression

```{r}

```

## $k$-NN

```{r}
library(FNN)          #for finding k nearest neighbor
library(fastDummies)
library(class)        #for computing k-NN numerical predictions
library(caret)
library(dplyr)        #for scaling
```


```{r}
diamonds_dummies <- diamonds
diamonds_dummies <- dummy_cols(diamonds_dummies, select_columns = c("cut", "color", "clarity"))
diamonds_dummies <- diamonds_dummies[,-c(2:4)]

#rename column to avoid issues with Neural net function
colnames(diamonds_dummies)[10] = "cut_Very_Good"
```


```{r}
#create data frames to normalize
knn_train <- diamonds_dummies[train.index,]
knn_valid <- diamonds_dummies[valid.index,]

knn_train_norm <- knn_train
knn_valid_norm <- knn_valid

# use preProcess() to normalize non-categorical variables
norm_values <- preProcess(knn_train[, c(1:7)], method=c("center", "scale"))

knn_train_norm[, c(1:7)] <- predict(norm_values, knn_train[, c(1:7)])
knn_valid_norm[,c(1:7)] <- predict(norm_values, knn_valid[, c(1:7)])
```

```{r}
# Compute k=1 nearest neighbor 
nn1 <- FNN::knn.reg(train = knn_train_norm[, -1],
          test = knn_train_norm[, -1],
          y= knn_train_norm[, 1],
          k = 1)

length(knn_train_norm[, -1])
length(knn_train_norm[, 1])


nn1
summary(nn1)

#display results
row.names(knn_train)[attr(nn1, "nn.index")]
nn1

```

```{r}
#mean square prediction error
mean((knn_valid_norm - nn1$pred) ^ 2)
```

```{r}

#diamonds_dummies$cut <- as.factor(diamonds_dummies$cut) # Factor the cut to five levels 

#diamonds_dummies$color <- as.factor(diamonds_dummies$color) # Factor the color to seven levels 

#diamonds_dummies$clarity <- as.factor(diamonds_dummies$clarity) # Factor the clarity to eight levels 

# Setting up train controls
#repeats = 3
#numbers = 10
#tunel = 10

#set.seed(111)
#x = trainControl(method = "repeatedcv",
#                 number = numbers,
#                 repeats = repeats,
#                 classProbs = TRUE,
#                 summaryFunction = twoClassSummary)

#model1 <- train(price~. , data = knn_train_norm, method = "knn",
#               preProcess = c("center","scale"),
#               trControl = x,
#               tuneLength = tunel)

# Summary of model
#model1
```

**Resource**:https://rstudio-pubs-static.s3.amazonaws.com/402600_a1efe79095644d389bf8f4c6d6c9ad1c.html

**Resource (code below)**: https://quantdev.ssri.psu.edu/sites/qdev/files/kNN_tutorial.html (REDID BASED ON NEW CODE)
```{r}
diamonds_knn <- diamonds
```

```{r}
# put outcome in its own object
price_outcome <- diamonds_knn %>% select(price)

# remove original from the data set
diamonds_knn <- diamonds_knn %>% select(-price)
```

```{r}
str(diamonds_knn)
```

```{r}
# Scale continuous variables
diamonds_knn %>%
   mutate_at(c(4,9), funs(c(scale(.))))
```

```{r}
# Create dummy variables
cut <- as.data.frame(dummy_cols(diamonds_knn$cut))
cut <- cut %>% select(-c[1,])
color <- as.data.frame(dummy_cols(diamonds_knn$color))
clarity <- as.data.frame(dummy_cols(diamonds_knn$clarity))
```

```{r}
# Combine dummies with original data set
diamonds_knn <- cbind(diamonds_knn, cut, color, clarity)
```

```{r}
# Remove original variables that had to be dummy coded
diamonds_knn <- diamonds_knn[,-c(1:3)]

head(diamonds_knn)
```
```{r}
#create data frames to normalize
knn_train <- diamonds_knn[train.index,]
knn_valid <- diamonds_knn[valid.index,]
```

```{r}
abs_outcome_train <- price_outcome[train.index, ]
abs_outcome_valid <- price_outcome[valid.index, ]
```

```{r}
reg_results <- knn.reg(knn_train, knn_valid, abs_outcome_train, k = 1)
print(reg_results)
```
**Resource**:https://stackoverflow.com/questions/46638731/error-when-fitting-knn-model

## Regression Tree

### Loading Libraries

```{r Libraries}
library(RColorBrewer)   #for graph colors
library(rattle)         #Graphical Data Interface
library(rpart)          #for regression trees
library(rpart.plot)     #for plot trees
library(forecast)       #for accuracy
library(ggplot2)        #for visualizations
library(caret)          #for variable importance
library(randomForest)   #for randomForest
library(gbm)            #for boosting
library(ipred)          #for bagging
library(xgboost)        #for fitting GBMs
library(vip)            #for variable importance
```

### Partitioning

```{r RT - Train, Valid, Test }
#Rename data specifically for regression trees
diamonds_tree <- diamonds

# Creating data tables Train, Valid and Test using the indexes for the regression tree section
Train <- diamonds[train.index, ]
Valid <- diamonds[valid.index, ]
```

### Regression Tree

```{r Default Regression Tree}
# Use rpart() to run tree on continuous response 
RegressTree <- rpart(price ~ ., 
              data = Train, 
              method = "anova") 

# Generates a cost complexity parameter table that provides the complexity parameter value
#summary(RegressTree)

# Plots a regression tree
fancyRpartPlot(RegressTree, caption = NULL, main = "Regression Tree", palettes = "YlGnBu", digits = -3)

# Count number of leaves 
length(RegressTree$frame$var[RegressTree$frame$var == "<leaf>"]) 
```

```{r ktable Importance - RegressTree}
# kable and kable_styling as before
# We multiply by 100, divide by the sum and round the percentages to 2 decimals
kable_styling(kable(round(100*RegressTree$variable.importance / sum(RegressTree$variable.importance), 2), col.names = "Importance %"), full_width = FALSE) %>% kable_classic() 
```

```{r Error Details on Default Tree - Valid}
# Predict errors using accuracy() 
tree_aa1 <- forecast::accuracy(predict(RegressTree, Valid), Valid$price)

kable(tree_aa1) %>% kable_classic() 
```

```{r Default Tree Predictions}
# Predict the diamond price with validation
pred_Diamond_test <- predict(RegressTree, newdata = Valid)

# Display first 14 observations
head(pred_Diamond_test,14)
```

### 'Exlcusion' Regression Tree 

```{r Default Regression Tree Minus Variables}
RegressTree2 <- rpart(price ~ length+width+depth+carat+cut+color+clarity, 
              data = Train, 
              method = "anova") 

# Generates a cost complexity parameter table that provides the complexity parameter value
#summary(RegressTree2)

# Plots a regression tree
fancyRpartPlot(RegressTree2, caption = NULL, main = "Exclusion Regression Tree", palettes = "YlGnBu", digits = -3)
```

```{r ktable Importance - RegressTree2}
# kable and kable_styling as before
# We multiply by 100, divide by the sum and round the percentages to 2 decimals
kable_styling(kable(round(100*RegressTree2$variable.importance / sum(RegressTree2$variable.importance), 2), col.names = "Importance %"), full_width = FALSE) %>% kable_classic() 
```

```{r Error Details on RegressTree2 - Valid}
# Predict errors using accuracy() 
tree_aa2 <- forecast::accuracy(predict(RegressTree2, Valid), Valid$price)

kable(tree_aa2) %>% kable_classic() 
```

```{r Default Tree Predictions2}
# Predict the diamond price with validation
pred_Diamond_test2 <- predict(RegressTree2, newdata = Valid)

# Display first 14 observations
head(pred_Diamond_test2,14)
```

```{r CP Value}
#Get the lowest CP value from CP table
min.xerror <- RegressTree2$cptable[which.min(RegressTree2$cptable[,"xerror"]),"CP"]

min.xerror
```

```{r Plot Cp}
#Plot the optimal Cp value
plotcp(RegressTree2)
```

### Pruned Regression Tree

```{r Pruned Regression Tree Minus Variables}
RegressTree_pruned <- prune(RegressTree2, cp = min.xerror) 

# Draw the prune tree
fancyRpartPlot(RegressTree_pruned, caption = NULL, main = "'Pruned' Regression Tree", palettes = "YlGnBu", digits = -3)
```

```{r Error Details on Prune_RegressTree2 - Valid}
# Predict errors using accuracy() 
tree_aa3 <- forecast::accuracy(predict(RegressTree_pruned, Valid), Valid$price)

kable(tree_aa3) %>% kable_classic() 
```

### Boosted Tree

```{r}
set.seed(111)
bst <- xgboost(
data = data.matrix(subset(Train, select = -price)),
label = Train$price,
objective = "reg:squarederror",
nrounds = 100,
max_depth = 5,
eta = 0.3,
verbose = 0 # suppress printing
)

vip::vip(bst, aesthetics = list(fill = "#F9D53E")) +
  ggtitle("XGBoosted Tree Variable Importance") + # Title name
  xlab("Variable") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines

bst

```

**Source**: https://cran.r-project.org/web/packages/vip/vignettes/vip-introduction.pdf

```{r}
min(bst$test.error.mean)
```

```{r Boosted Tree 10}
# Boosted tree
set.seed(111)
tree_boost10 <- gbm(price ~., data = Train, distribution = "gaussian", n.trees = 10, interaction.depth = 6, shrinkage = 0.01)

# Interaction Depth specifies the maximum depth of each tree( i.e. highest level of variable interactions allowed while training the model). Default is 1.
# Shrinkage is considered as the learning rate. It is used for reducing, or shrinking, the impact of each additional fitted base-learner (tree).  It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. Default is 0.1
#n.trees: Integer specifying the total number of trees to fit. This is equivalent to the number of iterations and the number of basis functions in the additive expansion. Default is 100.


tree_boost10

vip::vip(tree_boost10, aesthetics = list(fill = "#F9D53E")) +
  ggtitle("Boosted Tree Variable Importance") + # Title name
  xlab("Variable") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines
```

```{r Error Details on tree_boost10 - Valid}
# Predict errors using accuracy() 
tree_aa4 <- forecast::accuracy(predict(tree_boost10, Valid), Valid$price)

kable(tree_aa4) %>% kable_classic() 
```

```{r Boosted Tree 30}
# Boosted tree
set.seed(111)
tree_boost30 <- gbm(price ~., data = Train, distribution = "gaussian", n.trees = 30)

vip::vip(tree_boost30, aesthetics = list(fill = "#F9D53E")) +
  ggtitle("Boosted Tree Variable Importance") + # Title name
  xlab("Variable") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines

tree_boost30
```

```{r Error Details on tree_boost30}
# Predict errors using accuracy() 
tree_aa5 <- forecast::accuracy(predict(tree_boost30, Train), Train$price)

kable(tree_aa5) %>% kable_classic() 
```

```{r Boosted Tree 100}
# Boosted tree
set.seed(111)
tree_boost100 <- gbm(price ~., data = Train, distribution = "gaussian", cv.folds = 3)

# cv.folds: Number of cross-validation folds to perform. If cv.folds>1 then gbm, in addition to the usual fit, will perform a cross-validation, calculate an estimate of generalization error returned in cv.error.

vip::vip(tree_boost100, aesthetics = list(fill = "#F9D53E")) +
  ggtitle("Boosted Tree Variable Importance") + # Title name
  xlab("Variable") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines

tree_boost100
```

```{r Error Details on tree_boost100}
# Predict errors using accuracy() 
tree_aa6 <- forecast::accuracy(predict(tree_boost100, Valid), Valid$price)

kable(tree_aa6) %>% kable_classic()
```

```{r Boosted Tree 100 with oob}
ntree_opt_oob <- gbm.perf(tree_boost100, method = "OOB")
ntree_opt_oob

# Indicate the method used to estimate the optimal number of boosting iterations. method = "OOB" computes the out-of-bag estimate and method = "test" uses the test (or validation) dataset to compute an out-of-sample estimate. method = "cv" extracts the optimal number of iterations using cross-validation if gbm was called with cv.folds > 1.
```

```{r Predict Boosting}
pred_boost <- predict.gbm(tree_boost100, newdata = Valid)
```

### Bagging Tree

```{r Bagged Tree}
# Bagged tree
set.seed(111)
tree_bagging <- bagging(price ~., data = Train, coob = TRUE)
tree_bagging
```

```{r Plot IV}
pred_imp <- varImp(tree_bagging)

#sort variable importance descending
VI_plot <- pred_imp[order(pred_imp$Overall, decreasing=FALSE),]

#visualize variable importance with horizontal bar plot
barplot(sort(pred_imp$Overall, decreasing = FALSE),
        names.arg=rownames(pred_imp),
        horiz=TRUE,
        col="#F9D53E", las = 1)

# Add a main title and bottom and left axis labels
title("Variable Importance ", xlab="Importance")
```

```{r Errors Bagging}
tree_aa7 <- forecast::accuracy(predict(tree_bagging, Valid), Valid$price)

kable(tree_aa7) %>% kable_classic()
```

```{r}
pred_bagged <- predict(tree_bagging, newdata = Valid)

head(pred_bagged,14)
```

### RandomForest

```{r randomForest}
options(scipen = 9999)

# Create randomForest
set.seed(111)
rdf_model <- randomForest(price~ ., ntree= 60, data = Train)
plot(rdf_model)
```

```{r Optimal Random Trees}
# Number of trees with lowest MSE
# Compared with 80 and 100 trees, the trade-off with both of those were small so we stuck with 60 trees
which.min(rdf_model$mse)
```

```{r}
options(scipen = 9999)

# Create variable importance chart
vip::vip(rdf_model, aesthetics = list(fill = "#F9D53E")) +
  ggtitle("RandomTree Variable Importance") + # Title name
  xlab("Variable") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines
```

```{r}
tree_aa8 <- forecast::accuracy(predict(rdf_model, Valid), Valid$price)

kable(tree_aa8) %>% kable_classic()
```

### Create Regression Tree Summary Table

```{r}
Tree_res <- data.frame("Model_Tree" = c("Regression Tree",
                                 "Exclusion RT",
                                 "Pruned RT",
                                 "Boost10",
                                 "Boost30",
                                 "Boost100",
                                 "Bagging",
                                 "RandomForest"),
                     "ME" = "",
                     "RMSE" = "",
                     "MAE"= "",
                     "MPE" = "",
                     "MAPE" = "")

rownames(Tree_res) <- Tree_res$Model_Tree
Tree_res$Model_Tree = NULL

str(round(get(paste("tree_aa", "1", sep ="")),2))

for (i in 1:8) {
  Tree_res[i,]= round(get(paste("tree_aa", i, sep ="")),2)
}

kable_styling(kable(Tree_res, full_width = FALSE))

barplot(RMSEplotdata, col = "#F9D53E")
RMSEplotdata <- t(Tree_res$RMSE)
colnames(RMSEplotdata) <- t(rownames(Tree_res))

```

## NeuralNetworks


## Ensembles

```{r}

```

# Model Performance Summary

# Conclusions

