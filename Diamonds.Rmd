---
title: "Diamonds"
author: "Francisco Arrieta, Emily Schmidt and Lucia Camenisch"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float:
      collapsed: no
    number_sections: yes
    theme: cosmo
---

```{=html}
<style>
body{
  color: #2F91AE;
  background-color: #F2F2F2;
}
pre{
  background-color: #96EAE3;
}
pre:not([class]){
  background-color: #15DDD8;
}
.toc-content{
  padding-left: 10px;
  padding-right: 10px;
}
.col-sm-8 {
  width: 75%;
}
code {
  color: #333333;
  background-color: #96EAE3;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.align = "center")
```

```{r Colors, include=FALSE}
#Aqua =         "#15DDD8"
#Dark Blue =    "#2F91AE"
#Yellow =       "#F9D53E"
#Light Gray =   "#F2F2F2"
#Light Aqua =   "#96EAE3"
```

```{r Libraries}
library(data.table)     #for reading data.tables
library(ggplot2)        #for making graphs
library(tidyr)          #for changing the shape and hierarchy of a data set
library(ellipse)        #for mapping correlation
library(e1071)          #for skewness
library(caret)          #for preProcess() and accuracy()
library(fastDummies)    #for creating dummies
library(forecast)       # for accuracy() measures
library(kableExtra)     #for more elaborate tables
# library(GGally)         #for making graphs
# library(dplyr)          #for data manipulation
# library(DataExplorer)   #for graphing missing value percentages
# library(car)            #for statistic functions


source("VIF.R")
source("ProcStep.R")
source("GlobalCrit.R")
options(scipen = 999)
```

# Data Exploration 

```{r Import Data}
diamonds <- fread("diamonds.csv") # Load your data, diamonds.csv

diamonds$V1 <- NULL # Remove column 'V1' as it is similar to an ID variable - no additional meaning derived

# Rename columns for more precise names
colnames(diamonds)[5] <- "depth_ratio" # depth to depth_ratio
colnames(diamonds)[8] <- "length" # x to length
colnames(diamonds)[9] <- "width"  # y to width
colnames(diamonds)[10] <- "depth" # z to depth
```


```{r categorical-as-factors}
# Review unique values for cut
unique(diamonds$cut)
# Factor the cut to five levels
diamonds$cut <- as.factor(diamonds$cut)
# Ordered from worst to best
diamonds$cut <- ordered(diamonds$cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))

# Review unique values for color
unique(diamonds$color)
# Factor the color to seven levels 
diamonds$color <- as.factor(diamonds$color) 
# Ordered from worst to best
diamonds$color <- ordered(diamonds$color, levels = c("J", "I", "H", "G", "F", "E", "D"))

# Review unique values for clarity
unique(diamonds$clarity)
# Factor the clarity to eight levels 
diamonds$clarity <- as.factor(diamonds$clarity)
# Ordered from worst to best
diamonds$clarity <- ordered(diamonds$clarity,
                            levels = c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
```


```{r removing-rows}
# Remove values of 0 for for dimensions which includes zeros in length and width
nrow(diamonds[depth %in% 0,])        # Remove 20 rows due to depth = 0.0
diamonds <- diamonds[depth > 0, ]    # Include only values with depth greater than zero

# Create formula to check the absolute value of length to width, comparison 
diamonds[, subtraction := abs(length - width)]
# Remove 2 rows due their extreme subtraction value (~59 and ~26)
nrow(diamonds[subtraction>10,])
# Include only values with subtraction less than ten
diamonds <- diamonds[subtraction <= 10, ]

diamonds[, depth_check := round(100*(2*depth)/((length + width)), 1)]
diamonds[, diff := abs(depth_check-depth_ratio)]
# treshold at 0.3? anastasia
nrow(diamonds[diff > 0.3,]) # we remove 253 rows
diamonds <- diamonds[diff <= 0.3,]
# hist(diamonds[diff >= 0.4 & diff < 1, diff], breaks = 50)

# Removed created columns needed to clean the data
diamonds[, subtraction := NULL]
diamonds[, depth_check := NULL]
diamonds[, diff := NULL]
# Total rows removed: 275 observations
```


```{r ordering cols}
# Reorder data table to group like variable types 
diamonds <- diamonds[, c(7, 2:4, 1, 8:10, 5:6)]
```



```{r Histograms}
ggplot(gather(data = diamonds[, c(1, 5:10)]), aes(value)) +
  geom_histogram(aes(y = after_stat(density)),
#                 bins = 10,
                 color = "white",
                 fill = "#F9D53E") + # Creates bin sizing with colors
  geom_density(alpha = .2, fill = "#F9D53E") +
  facet_wrap(~ key, scales = "free") + # Converting the graphs into panels
  ggtitle("Quantitative Variable Analysis") + # Title name
  ylab("Count") + xlab("Value") + # Label names
  theme_classic() # A classic theme, with x and y axis lines and no grid lines
```

```{r Correlation}
# Create heatmap to show variable correlation
# Round the correlation coefficient to two decimal places
cormat <- round(cor(diamonds[, c(1, 5:10)]), 2)

# Use correlation between variables as distance
reorder_cormat <- function(cormat){ 
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
return(cormat)
}

# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)

# Keeping only upper triangular matrix
# upper_tri returns TRUE/FALSE for each coordinate (TRUE -> part of upper triangle)
# multiplying will thus keep the upper triangle values and set the others to 0
cormat <- cormat*upper.tri(cormat, diag = TRUE)
# Values of the lower triangle (0) are replaced by NA
cormat[cormat == 0] <- NA

# Melt the correlation matrix
cormat <- reshape2::melt(cormat, na.rm = TRUE)

# Create a ggheatmap with multiple characteristics 
ggplot(cormat, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "#15DDD8", high = "#F9D53E", mid = "white",
                       midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") +
  ggtitle("Correlation Heatmap") + # Title name
  theme_minimal() + # Minimal theme, keeps in the lines
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
  coord_fixed() +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2)

rm(cormat, reorder_cormat)
```

```{r Correlation Plot}
plotcorr(cor(diamonds[, -c(2:4)]), col = "#F9D53E",
         main = "Pearson correlation ellipses for numerical variables")
```

```{r train-valid-test-indexes}
# set seed for reproducing the partition
set.seed(111)

# generating training set index
train.index <- sample(c(1:nrow(diamonds)), 0.5*nrow(diamonds))
# generating validation set index taken from the complementary of training set
valid.index <- sample(setdiff(c(1:nrow(diamonds)), train.index), 0.3*nrow(diamonds))
# defining test set index as complementary of (train.index + valid.index)
test.index <- as.integer(setdiff(row.names(diamonds), union(train.index, valid.index)))
```


## Linear Regression


```{r train-valid}
# creating training and validation sets
Train_lr <- diamonds[train.index, ]
Valid_lr <- diamonds[valid.index, ]
```


```{r vif}
# using the VIF function from statistical modelling to check multicollinearity of predictors
VIF(y = diamonds$price, matx = diamonds[, -c(1)])
# removing length, width and depth and computing VIF without them
VIF(y = diamonds$price, matx = diamonds[, -c(1, 6, 7, 8)])

# plotting correlation ellipses of numerical variables with length, width and depth removed
plotcorr(cor(diamonds[, -c(2:4, 6:8)]), col = "#F9D53E",
         main = "Pearson correlation ellipses for numerical variables")
```

```{r skewness}
# applying the skewness() function of every numerical variable from our training set
sapply(Train_lr[, c(1, 5:10)], skewness)

# logarithmic transformation on price, carat and table
Train_lr$price <- log(Train_lr$price)
Train_lr$carat <- log(Train_lr$carat)
Train_lr$table <- log(Train_lr$table)

# recomputing the skewness of numerical variables to see the improvement
sapply(Train_lr[, c(1, 5:10)], skewness)

# transforming in the validation set as well
Valid_lr$price <- log(Valid_lr$price)
Valid_lr$carat <- log(Valid_lr$carat)
Valid_lr$table <- log(Valid_lr$table)
```

```{r hist-unskewed}
# computing the histograms of numerical variables now that they are unskewed

ggplot(gather(data = Train_lr[, c(1, 5:10)]), aes(value)) + # numerical vars of training set
  geom_histogram(aes(y = after_stat(density)),              # making histograms with color params
                 color = "white",
                 fill = "#F9D53E") +             
  geom_density(alpha = .2, fill = "#F9D53E") +              # making density lines
  facet_wrap(~ key, scales = "free") +                      # multiple plots with facet
  labs(title = "Histograms of numerical variables",         # labels of the plot
       x = "Count",
       y = "Value") +
  theme_classic()                                           # aesthetic theme
```

```{r normalizing}
# we compute the the mean and std values based on training data (for numerical variables)
norm.values <- preProcess(Train_lr[, c(1, 5:10)], method=c("center", "scale"))

# we standardize the training and validation data
Train_lr[, c(1, 5:10)] <- predict(norm.values, Train_lr[, c(1, 5:10)])
Valid_lr[, c(1, 5:10)] <- predict(norm.values, Valid_lr[, c(1, 5:10)])
```


```{r LM-complete}
# we compute the linear model using all predictors and display its summary
LM_complete = lm(price ~. , data = Train_lr)
summary(LM_complete)
```

```{r iterative-search-complete}
# we use iterative search algorithms on the complete model: forward, backward and stepwise
# we display summaries of the three models obtained with iterative methods

LM_forward_complete = step(LM_complete, direction = "forward")
summary(LM_forward_complete)

LM_backward_complete = step(LM_complete, direction = "backward")
summary(LM_backward_complete)

LM_stepwise_complete = step(LM_complete, direction = "both")
summary(LM_stepwise_complete)
```

```{r LM_CpAIC_complete}
# we use the GlobalCrit function from statistical modelling to reduce the number of predictors
# using global criterions: Mallows's Cp and AIC
GlobalCrit(LM_complete)

# we compute the linear model obtained with global methods and display its summary
LM_CpAIC_complete = lm(price ~ . , data = Train_lr[, c(1:6, 8, 9)])
summary(LM_CpAIC_complete)
```




```{r LM-minus-corr}
# we define a training set without the correlated predictors (length, width, depth)
Train_minus_corr <- Train_lr[, -c(6:8)]

# we compute the linear model without correlated predictors and display its summary
LM_minus_corr = lm(price ~ ., data = Train_minus_corr)
summary(LM_minus_corr)
```

```{r iterative-search-minus-corr}
# we use iterative search algorithms on the model without corr: forward, backward and stepwise
# we display summaries of the three models obtained with iterative methods

LM_backward_minus_corr = step(LM_minus_corr, direction = "backward")
summary(LM_backward_minus_corr)

LM_forward_minus_corr = step(LM_minus_corr, direction = "forward")
summary(LM_forward_minus_corr)

LM_stepwise_minus_corr = step(LM_minus_corr, direction = "both")
summary(LM_stepwise_minus_corr)
```

```{r LM_CpAIC_minus_corr}
# we use the GlobalCrit function from statistical modelling to reduce the number of predictors
# using global criterions: Mallows's Cp and AIC
GlobalCrit(LM_minus_corr)

# we compute the linear model obtained with global methods and display its summary
LM_CpAIC_minus_corr = lm(price ~ . , data = Train_lr[, c(1:5, 9)])
summary(LM_CpAIC_minus_corr)
```

```{r predictors-table}
# we display a table of the predictors used in each model using kable()
# kable_styling() controls the parameter fullwidth for the total width of table

kable_styling(
  kable(
    # the data to display in the table: model names and crosses for presence of predictor
    data.table(Model = c("LM_complete", "LM_forward_complete", "LM_backward_complete",
                       "LM_stepwise_complete", "LM_CpAIC_complete",
                       "LM_minus_corr", "LM_forward_minus_corr", "LM_backward_minus_corr",
                       "LM_stepwise_minus_corr", "LM_CpAIC_minus_corr"),
             Cut           = c("X","X","X","X","X","X","X","X","X","X"),
             Color         = c("X","X","X","X","X","X","X","X","X","X"),
             Clarity       = c("X","X","X","X","X","X","X","X","X","X"),
             Carat         = c("X","X","X","X","X","X","X","X","X","X"),
             Length        = c("X","X","X","X","X"," "," "," "," "," "),
             Width         = c("X","X"," "," "," "," "," "," "," "," "),
             Depth         = c("X","X","X","X","X"," "," "," "," "," "),
             `Depth Ratio` = c("X","X","X","X","X","X","X","X","X","X"),
             Table         = c("X","X"," "," "," ","X","X"," "," "," ")
             ),
    align = 'lccccccccc',                               # alignment of each column
    caption = "Predictors used in each linear model"),  # caption of the table
  full_width = FALSE)                                   # table isn't full width of page
```

In both cases, the forward selection doesn't discard any variables, whereas backward, stepwise and global selections all choose the same model with less variables than initially.

Thus, we have four different models emerging. We will keep `LM_complete`, `LM_CpAIC_complete`, `LM_minus_corr` and `LM_CpAIC_minus_corr` and remove the other models which are duplicates.

```{r removing-models}
# we remove redundant models
rm(LM_forward_complete, LM_backward_complete, LM_stepwise_complete, LM_forward_minus_corr, LM_backward_minus_corr, LM_stepwise_minus_corr)
```


```{r predictions-table}
# predicting prices of validation set on the validation data

# we create a data table of predictions which contains predictions for the four models
# predictions are computed on validation set using predict()
LM_Predictions =
  data.table(
    LM_complete_pred = predict(object = LM_complete, newdata = Valid_lr),
    LM_CpAIC_complete_pred = predict(object = LM_CpAIC_complete, newdata = Valid_lr),
    LM_minus_corr_pred = predict(object = LM_minus_corr, newdata = Valid_lr),
    LM_CpAIC_minus_corr_pred = predict(object = LM_CpAIC_minus_corr, newdata = Valid_lr)
    )
```


```{r rescaling-predictions}
# we have to scale back the price, to do so we fetch the mean and std value from norm.values

# we display all means and stds
norm.values$mean
norm.values$std

# fetching for price
mean_price = norm.values$mean[1]
std_price = norm.values$std[1]

# scaling back (Y*mu + sigma), then exp() (we had transformed price with a log for skewness)
LM_Predictions = LM_Predictions*std_price + mean_price
LM_Predictions = exp(LM_Predictions)
```


```{r real-prices}
# taking real prices of validation data from diamonds (which has not been touched -> original scale)
LM_Predictions[, real_prices := diamonds[valid.index, price]]
```


```{r models-accuracy-measures}
# we compute accuracy measures for each model using accuracy() from the forecast package
# we convert each accuracy to a table (in order to put them together afterwards)

Acc1 = accuracy(object = LM_Predictions$LM_complete_pred, x = LM_Predictions$real_prices)
Acc1 = as.data.table(Acc1)
Acc2 = accuracy(object = LM_Predictions$LM_CpAIC_complete_pred, x = LM_Predictions$real_prices)
Acc2 = as.data.table(Acc2)
Acc3 = accuracy(object = LM_Predictions$LM_minus_corr_pred, x = LM_Predictions$real_prices)
Acc3 = as.data.table(Acc3)
Acc4 = accuracy(object = LM_Predictions$LM_CpAIC_minus_corr_pred, x = LM_Predictions$real_prices)
Acc4 = as.data.table(Acc4)

# we create a list of accuracy measures
Accs = list(Acc1, Acc2, Acc3, Acc4)
# we use rbindlist() from the data.table package to stack the four tables on top of eachother
Accs = rbindlist(Accs)
# now that Accs is created we can remove the four tables
rm(Acc1, Acc2, Acc3, Acc4)
# we add a column to Accs with the model names
Accs[, Model := c("LM_complete", "LM_CpAIC_complete", "LM_minus_corr", "LM_CpAIC_minus_corr")]
# we put the Model column first
Accs <- Accs[, c(6, 1:5)]

# displaying Accs as a table with kable() and adding a caption
# kable_styling controls width of table
kable_styling(kable(Accs, caption = "Accuracy measures of linear models"), full_width = FALSE)
```



