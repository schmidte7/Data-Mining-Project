---
title: "Diamonds"
author: "Francisco Arrieta, Emily Schmidt and Lucia Camenisch"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true             # creating a table of contents (toc)
    toc_float: 
      collapsed: false    # toc does not collapse and is shown as a sidebar (toc_float)
    number_sections: true # document sections are numbered
    theme: cosmo
---

```{=html}
<style>
body{
  color: #2F91AE;
  background-color: #F2F2F2;
}
pre{
  background-color: #96EAE3;
}
pre:not([class]){
  background-color: #15DDD8;
}
.toc-content{
  padding-left: 10px;
  padding-right: 10px;
}
.col-sm-8 {
  width: 75%;
}
code {
  color: #333333;
  background-color: #96EAE3;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.align = "center")
```

```{r Colors, include=FALSE}
#Aqua =         "#15DDD8"
#Dark Blue =    "#2F91AE"
#Yellow =       "#F9D53E"
#Light Gray =   "#F2F2F2"
#Light Aqua =   "#96EAE3"
```

```{r Libraries}
library(data.table)     #for reading data.tables
# library(kableExtra)     #for more elaborate tables
# library(ggplot2)        #for making graphs
# library(GGally)         #for making graphs
# library(dplyr)          #for data manipulation
# library(tidyr)          #for changing the shape and hierarchy of a data set
# library(DataExplorer)   #for graphing missing value percentages
# library(car)            #for statistic functions
# library(ellipse)        #for mapping correlation

#source("VIF.R")
```

# Data Exploration 

```{r Import Data}
diamonds <- fread("diamonds.csv", sep=",", header = T) # Load your data, diamonds.csv

diamonds$V1 <- NULL # Remove column 'V1' as it is similar to an ID variable - no additional meaning derived

# Rename columns for more precise names
colnames(diamonds)[5] <- "depth_ratio" # depth to depth_ratio
colnames(diamonds)[8] <- "length" # x to length
colnames(diamonds)[9] <- "width"  # y to width
colnames(diamonds)[10] <- "depth" # z to depth
```



```{r Variables check}
# carat no problems
unique(diamonds$cut) # Review unique values for cut
diamonds$cut <- as.factor(diamonds$cut) # Factor the cut to five levels 
diamonds$cut <- ordered(diamonds$cut, levels = c("Fair", "Good", "Very Good", "Premium", "Ideal")) # Ordered from worst to best

unique(diamonds$color) # Review unique values for color
diamonds$color <- as.factor(diamonds$color) # Factor the color to seven levels 
diamonds$color <- ordered(diamonds$color, levels = c("J", "I", "H", "G", "F", "E", "D")) # Ordered from worst to best

unique(diamonds$clarity) # Review unique values for clarity
diamonds$clarity <- as.factor(diamonds$clarity) # Factor the clarity to eight levels 
diamonds$clarity <- ordered(diamonds$clarity, levels = c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF")) # Ordered from worst to best

# table is ok

# price is ok

# Remove values of 0 for for dimensions which includes zeros in length and width
nrow(diamonds[depth %in% 0,]) # Remove 20 rows due to depth = 0.0
diamonds <- diamonds[depth > 0, ] # Include only values with depth greater than zero

# Create formula to check the absolute value of length to width, comparison 
diamonds[, subtraction := abs(length - width)]
nrow(diamonds[subtraction>10,]) # Remove 2 rows due their extreme subtraction value (~59 and ~26)
diamonds <- diamonds[subtraction <= 10, ] # Include only values with subtraction less than ten

diamonds[, depth_check := round(100*(2*depth)/((length + width)), 1)]
diamonds[, diff := abs(depth_check-depth_ratio)]
# treshold at 0.3? anastasia
nrow(diamonds[diff > 0.3,]) # we remove 268 rows
diamonds <- diamonds[diff <= 0.3,]
# hist(diamonds[diff >= 0.4 & diff < 1, diff], breaks = 50)

# Removed created columns needed to clean the data
diamonds[, subtraction := NULL]
diamonds[, depth_check := NULL]
diamonds[, diff := NULL]
# Total rows remove: 275 observations
```


```{r ordering cols}
# Reorder data table to group like variable types 
diamonds <- diamonds[, c(7, 2:4, 1, 8:10, 5:6)]
```


```{r train-valid-test}
# set seed for reproducing the partition
set.seed(111)

# generating training set index
train.index <- sample(c(1:nrow(diamonds)), 0.5*nrow(diamonds))
# generating validation set index taken from the complementary of training set
valid.index <- sample(setdiff(c(1:nrow(diamonds)), train.index), 0.3*nrow(diamonds))
# defining test set index as complementary of (train.index + valid.index)
test.index <- as.integer(setdiff(row.names(diamonds), union(train.index, valid.index)))
# creating data tables Train, Valid and Test using the indexes
```



## NeuralNetworks

```{r Set Libraries}
# library(neuralnet)      #for constructing neural networks
# library(Metrics)        #for calculating RMSE
library(caret)          #for dummy variables
library(fastDummies)
# library(doParallel)     #for parallel computing of neural networks
```


```{r Dummy ariables}
#create dummy columns 
diamonds_dummies <- dummy_cols(diamonds, select_columns = c("cut", "color", "clarity"))
diamonds_dummies <- diamonds_dummies[,-c(2:4)]

#rename column to avoid issues with Neural net function
colnames(diamonds_dummies)[10] = "cut_Very_Good"
```

```{r Partitioning}
train_diamonds <- diamonds_dummies[train.index,]
valid_diamonds <- diamonds_dummies[valid.index,]
```


```{r Normalize}
# use preProcess() to normalize non-categorical variables
norm_values <- preProcess(train_diamonds[1:7,], method= c("range"))

train_diamonds_norm <- predict(norm_values, train_diamonds)
valid_diamonds_norm <- predict(norm_values, valid_diamonds)

```


```{r neural-network-tensorflow}
library(tensorflow)
library(keras)
library(magrittr)

# separating dependent variable from explanatory variables in x_train and y_train
# converting them to tensors (input type to be used by tensorflow)
# x_train <- train_diamonds_norm[, -c(1)]
# x_train <- as.array(unlist(x_train),
#                     dim = dim(x_train),
#                     dimnames = list(rownames(x_train), colnames(x_train)))
# x_train <- as_tensor(x_train, shape = dim(train_diamonds_norm[, -c(1)]))
# x_train2 <- as_tensor(train_diamonds_norm[, -c(1)])

x_train <- as_tensor(train_diamonds_norm[, -c(1)])
y_train <- as_tensor(train_diamonds_norm$price)
x_train
y_train

# same for validation set
x_valid <- as_tensor(valid_diamonds_norm[, -c(1)])
y_valid <- as_tensor(valid_diamonds_norm$price)
x_valid
y_valid

# creating NN model
# input_shape must be the number of input nodes (26)
# as seen in the book, we use the popular sigmoid activation function g(s) = 1/(1 + e^-s)

rm(model)
tf$random$set_seed(111)

model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
  layer_dense(26, activation = "sigmoid", name = "HiddenLayer") %>%  # 1st hidden layer (26 nodes)
  layer_dense(1, activation = "sigmoid", name = "outputLayer")       # output layer (1 node)

# We need to define an optimizer function, a loss function and a metric.
# The loss function is what the model will try to minimize at each step.
# Minimizing a function is called an optimization problem.
# Optimizer functions solve these optimization problems, they will update the weight and biases at
# each step of the algorithm. Optimizers have learning rates (they can be adjusted).
# The metric is NOT used for training the model but for judging its performance.

# Here, we choose the adam optimizer. Adam is an adaptative optimizer.
# This means it will automatically adjust the learning rate as it learns.
# Chosen after reading this article
# https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e

# We decide we want to minimize the mean squared error (MSE), so we choose it as loss function

# Our model will be judged based on its root mean squared error (RMSE), so we choose it as metric.

model %>% compile(
  optimizer = optimizer_adam(),
  loss = loss_mean_squared_error(),
  metric = metric_root_mean_squared_error()
  )

# we feed training and validation data to the model.
# batch_size and epochs are left at their default values, 32 and 10 (play around with them later?)
history <- model %>% fit(
    x = x_train,
    y = y_train,
    validation_data = c(x_valid, y_valid)
    )

# we print the summary of our model and plot it
# "shape" tells us the number of neurons per layer,
# "params" the number of parameters (weights + bias) per layer
# hidden layer has 26*26 = 676 weights (connecting 26 inputs to 26 nodes) and 26 bias (one for each
# node of the layer), totaling 702 = 676 + 26 params
# output layer has 26 weights (connecting 26 nodes to 1 node) and 1 bias (1 node as output),
# totaling 27 = 26 + 1 params
summary(model)
plot(model, show_shapes = TRUE, show_layer_names = TRUE)


# we can also summarise and plot history to have the plots of the evolution of loss and metric
# with respect to our epochs (10 by default)
summary(history)
plot(history)


# now we use model to predict on validation data set
# Question: since validation set is used by the algorithm, it might be best to look also at results
# on the test set? it will be exactly the same process as below.

library(forecast) # for accuracy() function

# prediction values of validation set
y_valid_pred <- predict(model, x_valid)

# fetching a and b from standardization for scaling back price
a <- norm_values$ranges[1,1]
b <- norm_values$ranges[2,1]

# scaling back price predictions
y_valid_pred <- (b-a)*y_valid_pred + a

# we have to change the class of y_valid_pred
class(y_valid_pred)
y_valid_pred <- as.numeric(y_valid_pred)
class(y_valid_pred)

# taking real values of validation set from original data (which wasn't standardized!)
y_valid_real <- diamonds[valid.index, price]

# copmuting accuracy measures of validation set
accuracy(object = y_valid_pred, x = y_valid_real)
```



```{r}
 #  layer1 = c(6,12,18,24,30)
 #  layer2 = c(6,12,18,24,30)
 #  layer3 = c(6,12,18,24,30)
 # 
 #  cv.folds = 5
 # 
 #  total.param.permutations = length(layer1) * length(layer2) * length(layer3)
 # 
 #  seeds <- vector(mode = "list", length = cv.folds + 1)
 #  set.seed(1)  
 #  for(i in 1:cv.folds) seeds[[i]]<- sample.int(n=1, total.param.permutations, replace = TRUE)
 #  seeds[[cv.folds + 1]]<-sample.int(1, 1, replace = TRUE) #for the last model
 # 
 #  nn.grid <- expand.grid(layer1 = layer1, layer2 = layer2, layer3 = layer3)
 # 
 #  cl <- makeCluster(detectCores()*0.5) # use 50% of cores only, leave rest for other tasks
 #  registerDoParallel(cl)
 # 
 #  train_control <- caret::trainControl(method = "cv" 
 #                                       ,number=cv.folds 
 #                                       ,seeds = seeds # user defined seeds for parallel processing
 #                                       ,verboseIter = TRUE
 #                                       ,allowParallel = TRUE
 #                                       )
 # 
 #  stopCluster(cl)
 #  registerDoSEQ()
 # 
 #  tic("Total Time to NN Training: ")
 #  set.seed(1)
 #  model.nn.caret = caret::train(form = formula,
 #                       data = scaled.train.data,
 #                       method = 'neuralnet',
 #                       tuneGrid = nn.grid,
 #                       trControl = train_control
 #                       )
 # toc()
```


```{r Parallel Computing}
# detectCores()
# getDoParWorkers()
# cl <- makeCluster(4)
# registerDoParallel(cl)
# set.seed(1)
# system.time(
# #create neural network 
# diaNeuralNet <- neuralnet(price ~ ., 
#                           data = train_diamonds_norm,
#                           linear.output = TRUE, 
#                           hidden = 1)
# )
# 
# 
# system.time(
#   model.nn.caret = caret::train(form = formula,
#                        data = train_diamonds_norm,
#                        method = 'neuralnet'
#                        )
# )
# 
# stopCluster(cl)
```


## Neural Network with Layers = 1 and Nodes = 2

```{r NN Layers 1 Nodes 2}
# # create neural network plot
# plot(diaNeuralNet, rep="best", show.weights = T, col.entry = "#00C1D5", col.hidden = "#387C2C", col.out = "#7F35B2", intercept = F)
# 
# # predict value

```


## Measuring errors (Layers = 1, Nodes = 2)

```{r RMSE Layers 1 Nodes 2}
# #a function to convert normalized values to original values
# valRescale <- function (x) { 
#   value <- (x * (max(train_diamonds$price)- min(diamonds$price))) + min(diamonds$price) 
#   return(value)
# }
# 
# # predict values of Data using Neural net
# dia_train_pred <- neuralnet::compute(diaNeuralNet, train_diamonds_norm)$net.result
# 
# 
# # Calculate Root Mean Square Error (RMSE) using Metrics package
# rs_actual_1 <- valRescale(train_diamonds_norm$price)
# rs_pred_1 <- valRescale(dia_train_pred)
# 
# #calculate RMSE
# RMSE_T_1 <- rmse(rs_actual_1, rs_pred_1)
# 
# #calculate for Validation data
# dia_valid_pred <- neuralnet::compute(diaNeuralNet, valid_diamonds_norm)$net.result
# 
# rs_actual_V_1 <- valRescale(valid_diamonds_norm$price)
# rs_pred_V_1 <- valRescale(dia_valid_pred)
# 
# RMSE_V_1 <- rmse(rs_actual_V_1, rs_pred_V_1)
# 
# #display Results
# RMSE_T_1
# RMSE_V_1
```


## Ensembles

```{r}

```

# Model Performance Summary

# Conclusions

